{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from multiprocessing import cpu_count\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2252 551\n"
     ]
    }
   ],
   "source": [
    "a = \"label_train_test.pkl\"\n",
    "with open(a, 'rb') as f:\n",
    "    label, train, test = pickle.load(f)\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# files 2706\n",
      "14 {'HSIL', 'HSV', 'TRI', 'VIRUS', 'EC', 'LSIL', 'CC', 'SCC', 'ASCH', 'AGC', 'ACTINO', 'NILM', 'ASCUS', 'FUNGI'}\n"
     ]
    }
   ],
   "source": [
    "data_path = './gnet2'\n",
    "all_data = os.listdir(data_path)\n",
    "print('# files', len(all_data))\n",
    "data = []\n",
    "all_labels = set()\n",
    "for file_ in all_data:\n",
    "    basename = file_\n",
    "    name = os.path.splitext(basename)[0]\n",
    "    data.append(os.path.join(data_path, basename))\n",
    "    label_ = label[name]\n",
    "    ls = label_.split('+')\n",
    "    for l in ls:\n",
    "        all_labels.add(l)\n",
    "    # print(name, label_)\n",
    "print(len(all_labels), all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpp</th>\n",
       "      <th>area</th>\n",
       "      <th>patches</th>\n",
       "      <th>patch_i</th>\n",
       "      <th>patch_label</th>\n",
       "      <th>patch_probability</th>\n",
       "      <th>detect_label</th>\n",
       "      <th>detect_probability</th>\n",
       "      <th>classify_label</th>\n",
       "      <th>classify_probability</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>HSIL-SCC_G</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>TRI</td>\n",
       "      <td>0.274466</td>\n",
       "      <td>TRI</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5686</td>\n",
       "      <td>20344</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>667</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0.455652</td>\n",
       "      <td>TRI</td>\n",
       "      <td>0.611174</td>\n",
       "      <td>TRI</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18504</td>\n",
       "      <td>16168</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1371</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0.919858</td>\n",
       "      <td>TRI</td>\n",
       "      <td>0.181044</td>\n",
       "      <td>TRI</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>31380</td>\n",
       "      <td>30986</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>652</td>\n",
       "      <td>TRI</td>\n",
       "      <td>0.989159</td>\n",
       "      <td>TRI</td>\n",
       "      <td>0.105297</td>\n",
       "      <td>TRI</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>17590</td>\n",
       "      <td>48734</td>\n",
       "      <td>32</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2336</td>\n",
       "      <td>SC</td>\n",
       "      <td>0.998384</td>\n",
       "      <td>SC</td>\n",
       "      <td>0.637666</td>\n",
       "      <td>SC</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>47816</td>\n",
       "      <td>14838</td>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1053</td>\n",
       "      <td>HSIL-SCC_G</td>\n",
       "      <td>0.999529</td>\n",
       "      <td>ACTINO</td>\n",
       "      <td>0.119227</td>\n",
       "      <td>ACTINO</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>25328</td>\n",
       "      <td>30742</td>\n",
       "      <td>160</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1040</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0.962150</td>\n",
       "      <td>HSIL-SCC_G</td>\n",
       "      <td>0.284843</td>\n",
       "      <td>HSIL_S</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>25548</td>\n",
       "      <td>19042</td>\n",
       "      <td>74</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>911</td>\n",
       "      <td>HSIL-SCC_G</td>\n",
       "      <td>0.992527</td>\n",
       "      <td>HSIL-SCC_G</td>\n",
       "      <td>0.275052</td>\n",
       "      <td>HSIL_B</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>22694</td>\n",
       "      <td>46670</td>\n",
       "      <td>150</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1855</td>\n",
       "      <td>HSIL-SCC_G</td>\n",
       "      <td>0.990912</td>\n",
       "      <td>HSIL-SCC_G</td>\n",
       "      <td>0.863593</td>\n",
       "      <td>HSIL_B</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>38498</td>\n",
       "      <td>37574</td>\n",
       "      <td>290</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1574</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0.782509</td>\n",
       "      <td>HSIL-SCC_G</td>\n",
       "      <td>0.451382</td>\n",
       "      <td>HSIL_S</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>34336</td>\n",
       "      <td>22272</td>\n",
       "      <td>64</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mpp  area  patches  patch_i patch_label  patch_probability detect_label  \\\n",
       "0  NaN   NaN      NaN       69  HSIL-SCC_G           0.464286          TRI   \n",
       "1  NaN   NaN      NaN      667      NORMAL           0.455652          TRI   \n",
       "2  NaN   NaN      NaN     1371      NORMAL           0.919858          TRI   \n",
       "3  NaN   NaN      NaN      652         TRI           0.989159          TRI   \n",
       "4  NaN   NaN      NaN     2336          SC           0.998384           SC   \n",
       "5  NaN   NaN      NaN     1053  HSIL-SCC_G           0.999529       ACTINO   \n",
       "6  NaN   NaN      NaN     1040      NORMAL           0.962150   HSIL-SCC_G   \n",
       "7  NaN   NaN      NaN      911  HSIL-SCC_G           0.992527   HSIL-SCC_G   \n",
       "8  NaN   NaN      NaN     1855  HSIL-SCC_G           0.990912   HSIL-SCC_G   \n",
       "9  NaN   NaN      NaN     1574      NORMAL           0.782509   HSIL-SCC_G   \n",
       "\n",
       "   detect_probability classify_label  classify_probability      x      y    w  \\\n",
       "0            0.274466            TRI              1.000000   5686  20344   24   \n",
       "1            0.611174            TRI              1.000000  18504  16168   22   \n",
       "2            0.181044            TRI              0.999999  31380  30986   32   \n",
       "3            0.105297            TRI              0.999995  17590  48734   32   \n",
       "4            0.637666             SC              0.999993  47816  14838   38   \n",
       "5            0.119227         ACTINO              0.999993  25328  30742  160   \n",
       "6            0.284843         HSIL_S              0.999986  25548  19042   74   \n",
       "7            0.275052         HSIL_B              0.999973  22694  46670  150   \n",
       "8            0.863593         HSIL_B              0.999967  38498  37574  290   \n",
       "9            0.451382         HSIL_S              0.999967  34336  22272   64   \n",
       "\n",
       "     h  \n",
       "0   22  \n",
       "1   22  \n",
       "2   40  \n",
       "3   26  \n",
       "4   36  \n",
       "5  164  \n",
       "6   64  \n",
       "7  226  \n",
       "8  214  \n",
       "9   54  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data[5])\n",
    "# patches = float(df.patches[df.patches.notnull()])\n",
    "# print(patches)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HSIL': 7, 'HSV': 8, 'TRI': 12, 'VIRUS': 8, 'EC': 5, 'CC': 4, 'SCC': 11, 'ASCH': 2, 'AGC': 1, 'ACTINO': 0, 'NILM': 10, 'LSIL': 9, 'ASCUS': 3, 'FUNGI': 6}\n",
      "{'HSIL': 1, 'HSV': 1, 'TRI': 1, 'VIRUS': 1, 'EC': 1, 'CC': 1, 'SCC': 1, 'ASCH': 1, 'AGC': 1, 'ACTINO': 1, 'NILM': 0, 'LSIL': 1, 'ASCUS': 1, 'FUNGI': 1}\n"
     ]
    }
   ],
   "source": [
    "tolerate = {\"AGC\":{\"AGC_A\", \"AGC_B\"}, \n",
    "            \"LSIL\":{\"ASCUS\", \"LSIL_E\", \"LSIL_F\"}, \n",
    "            \"ASCUS\":{\"ASCUS\", \"LSIL_E\", \"LSIL_F\"}, \n",
    "            \"HSIL-SCC_G\":{\"HSIL_B\", \"HSIL_M\", \"HSIL_S\", \"SCC_G\"}, \n",
    "            \"SCC_R\":{\"SCC_R\"}, \n",
    "            \"EC\":{\"EC\"}, \n",
    "            \"CC\":{\"CC\"}, \n",
    "            \"VIRUS\":{\"VIRUS\"}, \n",
    "            \"FUNGI\":{\"FUNGI\"}, \n",
    "            \"ACTINO\":{\"ACTINO\"}, \n",
    "            \"TRI\":{\"TRI\"}, \n",
    "            \"PH\":{\"PH\"}, \n",
    "            \"SC\":{\"SC\", \"RC\", \"MC\", \"GEC\"}}\n",
    "\n",
    "dtct_p = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "clas_p = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99, 0.999]\n",
    "header = [\"{}_{:.2f}_{:.3f}\".format(key, dp, cp) for key in tolerate for dp in dtct_p for cp in clas_p]\n",
    "header.sort()\n",
    "header_map = {key:i for i,key in enumerate(header)}\n",
    "\n",
    "all_labels = {'ACTINO':0, 'AGC':1, 'ASCH':2, 'ASCUS':3, 'CC':4, 'EC':5, 'FUNGI':6, 'HSIL':7, 'HSV':8, 'LSIL':9, 'NILM':10, 'SCC':11, 'TRI':12, 'VIRUS':8}\n",
    "bin_labels = {}\n",
    "for l in all_labels:\n",
    "    if l == 'NILM':\n",
    "        bin_labels[l] = 0\n",
    "    else:\n",
    "        bin_labels[l] = 1\n",
    "print(all_labels)\n",
    "print(bin_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# files: 2706\n",
      "One Job Done, Remaining Job Count: 48\n",
      "One Job Done, Remaining Job Count: 40\n",
      "One Job Done, Remaining Job Count: 32\n",
      "One Job Done, Remaining Job Count: 24\n",
      "One Job Done, Remaining Job Count: 16\n",
      "One Job Done, Remaining Job Count: 8\n",
      "One Job Done, Remaining Job Count: 0\n",
      "(2785, 1170) (2785,) (2785,)\n"
     ]
    }
   ],
   "source": [
    "def extract_oldfashion(f):\n",
    "    df = pd.read_csv(f)\n",
    "    features = [0 for i in range(len(header))]\n",
    "    # check if is empty\n",
    "    if df.yolo_cell_class.isnull().values.any():  \n",
    "        return features\n",
    "    # cross levelup features\n",
    "    for i,row in df.iterrows():\n",
    "        for dp in dtct_p:\n",
    "            for cp in clas_p:\n",
    "                if row['xcp_cell_class'] in tolerate[row['yolo_cell_class']]:\n",
    "                    if row['yolo_cell_class_det'] > dp and row['xcp_cell_class_det'] > cp:\n",
    "                        key = \"{}_{:.2f}_{:.3f}\".format(row['yolo_cell_class'], dp, cp)\n",
    "                        features[header_map[key]] += 1\n",
    "    return features\n",
    "\n",
    "def extract(f):\n",
    "    df = pd.read_csv(f)\n",
    "    features = [0 for i in range(len(header))]\n",
    "#     areas = [0.0 for i in range(len(header))]\n",
    "    # check if is empty\n",
    "    if df.detect_label.isnull().values.any():  \n",
    "        return features\n",
    "    # cross levelup features\n",
    "    for i,row in df.iterrows():\n",
    "        for dp in dtct_p:\n",
    "            for cp in clas_p:\n",
    "                if row['classify_label'] in tolerate[row['detect_label']]:\n",
    "                    if row['detect_probability'] > dp and row['classify_probability'] > cp:\n",
    "                        key = \"{}_{:.2f}_{:.3f}\".format(row['detect_label'], dp, cp)\n",
    "                        features[header_map[key]] += 1\n",
    "#                         areas[header_map[key]] += row['w'] * row['h']\n",
    "#     # average areas\n",
    "#     for i in range(len(header)):\n",
    "#         areas[i] /= features[i] if features[i] != 0 else 1.0\n",
    "    \n",
    "#     # patches balanced numbers\n",
    "#     try:\n",
    "#         patches = float(df.patches[df.patches.notnull()])\n",
    "#         features_pb = [f*2000/patches for f in features]\n",
    "#         features += features_pb\n",
    "#     except:\n",
    "#         print(f)\n",
    "#         features *= 2\n",
    "\n",
    "#     features += areas\n",
    "    return features\n",
    "\n",
    "def collect(data):\n",
    "    X = []\n",
    "    ya = []  # all labels\n",
    "    yb = []  # binary labels\n",
    "    names = []\n",
    "    for f in data:\n",
    "        features = extract(f)\n",
    "        basename = os.path.splitext(os.path.basename(f))[0]\n",
    "        ls = label[basename].split('+')\n",
    "        if sum(features) == 0:\n",
    "            continue\n",
    "        for l in ls:\n",
    "            a = all_labels[l]\n",
    "            b = bin_labels[l]\n",
    "            X.append(features)\n",
    "            ya.append(a)\n",
    "            yb.append(b)\n",
    "            names.append(f)\n",
    "    return X, ya, yb, names\n",
    "\n",
    "def worker():\n",
    "    files = data\n",
    "    random.shuffle(files)\n",
    "    random.shuffle(files)\n",
    "    print(\"# files:\", len(files))\n",
    "\n",
    "    X, ya, yb, names = [], [], [], []\n",
    "    \n",
    "    executor = ProcessPoolExecutor(max_workers=36)\n",
    "    tasks = []\n",
    "\n",
    "    batch_size = 50\n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch = files[i : i+batch_size]\n",
    "#         collect(batch)\n",
    "        tasks.append(executor.submit(collect, batch))\n",
    "\n",
    "    job_count = len(tasks)\n",
    "    for future in as_completed(tasks):\n",
    "        X_, ya_, yb_, names_ = future.result()  # get the returning result from calling fuction\n",
    "        X += X_\n",
    "        ya += ya_\n",
    "        yb += yb_\n",
    "        names += names_\n",
    "        job_count -= 1\n",
    "        if job_count % 8 == 0: \n",
    "            print(\"One Job Done, Remaining Job Count: %s\" % (job_count))\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    ya = np.asarray(ya)\n",
    "    yb = np.asarray(yb)\n",
    "    print(X.shape, ya.shape, yb.shape)\n",
    "    \n",
    "    return X, ya, yb, names\n",
    "\n",
    "X, ya, yb, names = worker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary classification\n",
      "accuracy: 0.8723\n",
      "NILM 184  recall = 0.8043 precision = 0.8087\n",
      "ABN 372  recall = 0.9059 precision = 0.9035\n",
      "\n",
      "multilabel classification\n",
      "accuracy: 0.7120\n",
      "ACTINO 8  recall = 0.8750 precision = 0.8750\n",
      "AGC 6  recall = 0.3333 precision = 0.5000\n",
      "ASCH 23  recall = 0.0435 precision = 0.1111\n",
      "ASCUS 82  recall = 0.4878 precision = 0.5970\n",
      "CC 35  recall = 0.6571 precision = 0.7931\n",
      "EC 7  recall = 0.4286 precision = 0.4286\n",
      "FUNGI 60  recall = 0.7333 precision = 0.7719\n",
      "HSIL 47  recall = 0.7234 precision = 0.7083\n",
      "VIRUS 2  recall = 1.0000 precision = 1.0000\n",
      "LSIL 69  recall = 0.7681 precision = 0.7571\n",
      "NILM 184  recall = 0.8859 precision = 0.7376\n",
      "SCC 13  recall = 0.9231 precision = 0.6316\n",
      "TRI 16  recall = 0.5625 precision = 0.8182\n"
     ]
    }
   ],
   "source": [
    "class RFESVM:\n",
    "    def __init__(self):\n",
    "        self.estimator = SVR(kernel=\"linear\")\n",
    "        self.selector = None\n",
    "        \n",
    "    def select(self, X, y, num_feature):\n",
    "        self.selector = RFE(self.estimator, num_feature, step=1)\n",
    "        self.selector = self.selector.fit(X, y)\n",
    "        selected_feature_indices = self.selector.support_ # ndarray of True/False\n",
    "        return selected_feature_indices\n",
    "\n",
    "def split(X, y, mode, test_size, seed):\n",
    "    random.seed(seed)\n",
    "    N = 2 if mode == \"bin\" else 13\n",
    "    idx = {i:[] for i in range(N)}\n",
    "    for i,c in enumerate(y):\n",
    "        idx[c].append(i)\n",
    "    idx_t, idx_v = [], []\n",
    "    for c,indices in idx.items():\n",
    "        n = len(indices)\n",
    "        idx_t += indices[:-int(n*test_size)]\n",
    "        idx_v += indices[-int(n*test_size):]\n",
    "    X_train = X[idx_t]\n",
    "    X_valid = X[idx_v]\n",
    "    y_train = y[idx_t]\n",
    "    y_valid = y[idx_v]\n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "def rfe(X_train, X_valid, y_train, y_valid, num_features):\n",
    "    rfe_svm = RFESVM()\n",
    "    selected_feature_indices = rfe_svm.select(X_train, y_train, num_features)\n",
    "    X_train = X_train[:, selected_feature_indices] # Select elements of numpy array via boolean mask array\n",
    "    X_valid = X_valid[:, selected_feature_indices]\n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "def evaluate(y_valid, y_pred, mode):\n",
    "    if mode == \"bin\":\n",
    "        cur_labels = {0:'NILM', 1:'ABN'}\n",
    "    else:\n",
    "        cur_labels = {value:key for key,value in all_labels.items()}\n",
    "    \n",
    "    results = {i:[0,0,0] for i in range(len(cur_labels))}  # tp, fn, fp\n",
    "    for t,p in zip(y_valid, y_pred):\n",
    "        if t == p:\n",
    "            results[t][0] += 1\n",
    "        else:\n",
    "            results[t][1] += 1\n",
    "            results[p][2] += 1\n",
    "    for i in range(len(results)):\n",
    "        labeli = cur_labels[i]\n",
    "        recall = results[i][0] / (results[i][0] + results[i][1]) if results[i][0] + results[i][1] != 0 else 0.0\n",
    "        precision = results[i][0] / (results[i][0] + results[i][2]) if results[i][0] + results[i][2] != 0 else 0.0\n",
    "        print(labeli, results[i][0] + results[i][1], ' recall = {:.4f}'.format(recall), 'precision = {:.4f}'.format(precision))\n",
    "        \n",
    "    \n",
    "def classify(mode=\"bin\"):  # mode = \"bin\" or \"all\"\n",
    "    seed = 2018\n",
    "    test_size = 0.2\n",
    "#     num_features = 1170\n",
    "\n",
    "    y = yb if mode == \"bin\" else ya\n",
    "#     X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "    X_train, X_valid, y_train, y_valid = split(X, y, mode, test_size, seed)\n",
    "    # X_train, X_valid, y_train, y_valid = rfe(X_train, X_valid, y_train, y_valid, num_features)\n",
    "\n",
    "    model = XGBClassifier(max_depth=15, \n",
    "                          n_jobs=24, \n",
    "                          subsample=0.8, \n",
    "                          colsample_bylevel=1,\n",
    "                          colsample_bytree=0.6, \n",
    "                          scale_pos_weight=1, \n",
    "                          n_estimators=500, \n",
    "                          min_child_weight=1, \n",
    "                          learning_rate=0.1, \n",
    "                          gamma=0,\n",
    "                          random_state=seed)\n",
    "    eval_set = [(X_train, y_train), (X_valid, y_valid)]\n",
    "    if mode == \"bin\":\n",
    "        eval_metric = [\"auc\", \"error\"]\n",
    "    else:\n",
    "        eval_metric = [\"merror\"]\n",
    "    model.fit(X_train, y_train, \n",
    "              early_stopping_rounds=50, \n",
    "              eval_metric=eval_metric, \n",
    "              eval_set=eval_set, \n",
    "              verbose=False)\n",
    "\n",
    "    y_pred = model.predict(X_valid)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    print(\"accuracy: {:.4f}\".format(accuracy))\n",
    "    evaluate(y_valid, y_pred, mode)\n",
    "    \n",
    "    return model\n",
    "\n",
    "print('binary classification')\n",
    "bin_model = classify(\"bin\")\n",
    "print()\n",
    "print('multilabel classification')\n",
    "all_model = classify(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_imap = {0:'NILM', 1:'ABN'}\n",
    "all_imap = {0: 'ACTINO', 1: 'AGC', 2: 'ASCH', 3: 'ASCUS', 4: 'CC', 5: 'EC', 6: 'CANDIDA', 7: 'HSIL', 8: 'HSV', 9: 'LSIL', 10: 'NILM', 11: 'SCC', 12: 'TRI'}\n",
    "\n",
    "with open(\"gnet2models.pkl\", 'wb') as f:\n",
    "    pickle.dump(bin_model, f)\n",
    "    pickle.dump(all_model, f)\n",
    "    pickle.dump(bin_imap, f)\n",
    "    pickle.dump(all_imap, f)\n",
    "\n",
    "\n",
    "# with open(\"gnet2models.pkl\", 'rb') as f:\n",
    "#     bin_model = pickle.load(f)\n",
    "#     all_model = pickle.load(f)\n",
    "#     bin_imap = pickle.load(f)\n",
    "#     all_imap = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diagnoser:\n",
    "    def __init__(self, pkl_file):\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            self.bin_model = pickle.load(f)\n",
    "            self.all_model = pickle.load(f)\n",
    "            self.bin_imap = pickle.load(f)\n",
    "            self.all_imap = pickle.load(f)\n",
    "    \n",
    "    def extract_old(self, csv_file):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        features = [0 for i in range(len(header))]\n",
    "        # check if is empty\n",
    "        if df.yolo_cell_class.isnull().values.any():  \n",
    "            return features\n",
    "        # cross levelup features\n",
    "        for i,row in df.iterrows():\n",
    "            for dp in dtct_p:\n",
    "                for cp in clas_p:\n",
    "                    if row['xcp_cell_class'] in tolerate[row['yolo_cell_class']]:\n",
    "                        if row['yolo_cell_class_det'] > dp and row['xcp_cell_class_det'] > cp:\n",
    "                            key = \"{}_{:.2f}_{:.3f}\".format(row['yolo_cell_class'], dp, cp)\n",
    "                            features[header_map[key]] += 1\n",
    "        features = np.array([features])\n",
    "        return features\n",
    "    \n",
    "    def extract_new(self, csv_file):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        features = [0 for i in range(len(header))]\n",
    "        # check if is empty\n",
    "        if df.detect_label.isnull().values.any():  \n",
    "            return features\n",
    "        # cross levelup features\n",
    "        for i,row in df.iterrows():\n",
    "            for dp in dtct_p:\n",
    "                for cp in clas_p:\n",
    "                    if row['classify_label'] in tolerate[row['detect_label']]:\n",
    "                        if row['detect_probability'] > dp and row['classify_probability'] > cp:\n",
    "                            key = \"{}_{:.2f}_{:.3f}\".format(row['detect_label'], dp, cp)\n",
    "                            features[header_map[key]] += 1\n",
    "        features = np.array([features])\n",
    "        return features\n",
    "    \n",
    "    def bin_predict(self, csv_file):\n",
    "        f = self.extract_new(csv_file)\n",
    "        p = self.bin_model.predict(f)[0]\n",
    "        l = self.bin_imap[p]\n",
    "        return l\n",
    "        \n",
    "    def all_predict(self, csv_file):\n",
    "        f = self.extract_new(csv_file)\n",
    "        p = self.all_model.predict(f)[0]\n",
    "        l = self.all_imap[p]\n",
    "        return l\n",
    "\n",
    "    def bin_and_all_predict(self, csv_file):\n",
    "        f = self.extract_new(csv_file)\n",
    "        pb = self.bin_model.predict(f)[0]\n",
    "        lb = self.bin_imap[pb]\n",
    "        pa = self.all_model.predict(f)[0]\n",
    "        la = self.all_imap[pa]\n",
    "        return lb, la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASCUS\n",
      "ABN ASCUS\n"
     ]
    }
   ],
   "source": [
    "pkl_file = \"gnet2models.pkl\"\n",
    "d = Diagnoser(pkl_file)\n",
    "\n",
    "csv_file = data[111]\n",
    "l = d.all_predict(csv_file)\n",
    "print(l)\n",
    "\n",
    "lb, la = d.bin_and_all_predict(csv_file)\n",
    "print(lb, la)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(mode=\"bin\"):  # mode = \"bin\" or \"all\"\n",
    "    seed = 2018\n",
    "    \n",
    "    y = yb if mode == \"bin\" else ya\n",
    "    num_folds = 5\n",
    "    X_train_folds = np.array_split(X, num_folds)\n",
    "    y_train_folds = np.array_split(y, num_folds)\n",
    "    \n",
    "    best = [0.0, None] # accuracy, i\n",
    "    for i in range(0, num_folds):\n",
    "        X_train = np.concatenate(X_train_folds[:i] + X_train_folds[i+1:])\n",
    "        y_train = np.concatenate(y_train_folds[:i] + y_train_folds[i+1:])\n",
    "        X_valid = X_train_folds[i]\n",
    "        y_valid = y_train_folds[i]\n",
    "\n",
    "        model = XGBClassifier(max_depth=15, \n",
    "                              n_jobs=24, \n",
    "                              subsample=0.8, \n",
    "                              colsample_bylevel=1,\n",
    "                              colsample_bytree=0.6, \n",
    "                              scale_pos_weight=1, \n",
    "                              n_estimators=500, \n",
    "                              min_child_weight=1, \n",
    "                              learning_rate=0.1, \n",
    "                              gamma=0,\n",
    "                              random_state=seed)\n",
    "        eval_set = [(X_train, y_train), (X_valid, y_valid)]\n",
    "        if mode == \"bin\":\n",
    "            eval_metric = [\"auc\", \"error\"]\n",
    "        else:\n",
    "            eval_metric = [\"merror\"]\n",
    "        model.fit(X_train, y_train, \n",
    "                  early_stopping_rounds=50, \n",
    "                  eval_metric=eval_metric, \n",
    "                  eval_set=eval_set, \n",
    "                  verbose=False)\n",
    "\n",
    "        y_pred = model.predict(X_valid)\n",
    "        accuracy = accuracy_score(y_valid, y_pred)\n",
    "        print(\"accuracy: {:.4f}\".format(accuracy))\n",
    "        \n",
    "        if accuracy > best[0]:\n",
    "            best = [accuracy, i]\n",
    "            \n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('binary classification')\n",
    "classify(\"bin\")\n",
    "print()\n",
    "print('multilabel classification')\n",
    "classify(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
