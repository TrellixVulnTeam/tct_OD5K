{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from multiprocessing import cpu_count\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR, SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train data: map file paths with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"../label_train_test.pkl\"\n",
    "with open(a, 'rb') as f:\n",
    "    label, train, test = pickle.load(f)\n",
    "print(len(train), len(test))\n",
    "\n",
    "data_path = '../train15/data2800'\n",
    "all_data = os.listdir(data_path)\n",
    "print('# files', len(all_data))\n",
    "data = []\n",
    "all_labels = set()\n",
    "for file_ in all_data:\n",
    "    basename = file_\n",
    "    name = os.path.splitext(basename)[0]\n",
    "    data.append(os.path.join(data_path, basename))\n",
    "    label_ = label[name]\n",
    "    ls = label_.split('+')\n",
    "    for l in ls:\n",
    "        all_labels.add(l)\n",
    "    # print(name, label_)\n",
    "print(len(all_labels), all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_file = '../roi_results.txt'\n",
    "area_map = {}\n",
    "with open(area_file, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        name, patches, area = line.strip().split()\n",
    "        name = os.path.splitext(name)[0]\n",
    "        patches = int(patches)\n",
    "        area = float(area)\n",
    "        area_map[name] = {'patches':patches, 'area':area}\n",
    "print(area_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file = '../test1521.xlsx'\n",
    "df_l = pd.read_excel(label_file)\n",
    "df_l.head(10)\n",
    "\n",
    "label_map = {}\n",
    "for i,row in df_l.iterrows():\n",
    "    label_map[row['case_no']] = row['old_label'].split('+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../gnet2/test1521'\n",
    "data = [os.path.join(data_path, f) for f in os.listdir(data_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = data[5]\n",
    "df = pd.read_csv(f)\n",
    "# area = float(df.area[df.area.notnull()])\n",
    "area = area_map[os.path.basename(f).split('_BATCH')[0]]['area']\n",
    "print(area)\n",
    "# patches = float(df.patches[df.patches.notnull()])\n",
    "# print(patches)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACTINO': 0, 'ASCUS': 3, 'NILM': 10, 'ASCH': 2, 'LSIL': 9, 'FUNGI': 6, 'VIRUS': 8, 'CANDIDA': 6, 'AGC': 1, 'EC': 5, 'CC': 4, 'TRI': 12, 'SCC': 11, 'HSIL': 7, 'HSV': 8}\n",
      "{'ACTINO': 1, 'ASCUS': 1, 'NILM': 0, 'ASCH': 1, 'LSIL': 1, 'FUNGI': 1, 'VIRUS': 1, 'CANDIDA': 1, 'EC': 1, 'HSV': 1, 'CC': 1, 'TRI': 1, 'AGC': 1, 'SCC': 1, 'HSIL': 1}\n"
     ]
    }
   ],
   "source": [
    "tolerate = {\"AGC\":{\"AGC_A\", \"AGC_B\"}, \n",
    "            \"LSIL\":{\"ASCUS\", \"LSIL_E\", \"LSIL_F\"}, \n",
    "            \"ASCUS\":{\"ASCUS\", \"LSIL_E\", \"LSIL_F\"}, \n",
    "            \"HSIL-SCC_G\":{\"HSIL_B\", \"HSIL_M\", \"HSIL_S\", \"SCC_G\"}, \n",
    "            \"SCC_R\":{\"SCC_R\"}, \n",
    "            \"EC\":{\"EC\"}, \n",
    "            \"CC\":{\"CC\"}, \n",
    "            \"VIRUS\":{\"VIRUS\", \"HSV\"}, \n",
    "            \"FUNGI\":{\"FUNGI\", \"CANDIDA\"}, \n",
    "            \"ACTINO\":{\"ACTINO\"}, \n",
    "            \"TRI\":{\"TRI\"}, \n",
    "            \"PH\":{\"PH\"}, \n",
    "            \"SC\":{\"SC\", \"RC\", \"MC\", \"GEC\"}}\n",
    "\n",
    "dtct_p = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "clas_p = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99, 0.999]\n",
    "header = [\"{}_{:.2f}_{:.3f}\".format(key, dp, cp) for key in tolerate for dp in dtct_p for cp in clas_p]\n",
    "header.sort()\n",
    "header_map = {key:i for i,key in enumerate(header)}\n",
    "\n",
    "all_labels = {'ACTINO':0, 'AGC':1, 'ASCH':2, 'ASCUS':3, 'CC':4, 'EC':5, \n",
    "              'FUNGI':6, 'CANDIDA':6, 'HSIL':7, 'VIRUS':8, 'HSV':8, \n",
    "              'LSIL':9, 'NILM':10, 'SCC':11, 'TRI':12}\n",
    "bin_labels = {}\n",
    "for l in all_labels:\n",
    "    if l == 'NILM':\n",
    "        bin_labels[l] = 0\n",
    "    else:\n",
    "        bin_labels[l] = 1\n",
    "print(all_labels)\n",
    "print(bin_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header_imap = {(key, dp, cp):header_map[\"{}_{:.2f}_{:.3f}\".format(key, dp, cp)] for key in tolerate for dp in dtct_p for cp in clas_p}\n",
    "# print(header_imap)\n",
    "\n",
    "# with open('header_imap.pkl', 'wb') as f:\n",
    "#     pickle.dump(header_imap, f)\n",
    "    \n",
    "# # # read pkl file\n",
    "# # with open('header_imap.pkl', 'rb') as f:\n",
    "# #     header_imap = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_oldfashion(f):\n",
    "    df = pd.read_csv(f)\n",
    "    features = [0 for i in range(len(header))]\n",
    "    # check if is empty\n",
    "    if df.yolo_cell_class.isnull().values.any():  \n",
    "        return features * 2\n",
    "    # cross levelup features\n",
    "    for i,row in df.iterrows():\n",
    "        for dp in dtct_p:\n",
    "            for cp in clas_p:\n",
    "                if row['xcp_cell_class'] in tolerate[row['yolo_cell_class']]:\n",
    "                    if row['yolo_cell_class_det'] > dp and row['xcp_cell_class_det'] > cp:\n",
    "                        key = \"{}_{:.2f}_{:.3f}\".format(row['yolo_cell_class'], dp, cp)\n",
    "                        features[header_map[key]] += 1\n",
    "                        \n",
    "    # area balanced numbers\n",
    "    try:\n",
    "        area_mark = 2850000000\n",
    "        # area = float(df.area[df.area.notnull()])\n",
    "        area = area_map[os.path.basename(f).split('_BATCH')[0]]['area']\n",
    "        features_ab = [f*area_mark/area for f in features]\n",
    "        features += features_ab\n",
    "    except:\n",
    "        print(f)\n",
    "        features *= 2\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract(f):\n",
    "    df = pd.read_csv(f)\n",
    "    features = [0 for i in range(len(header))]\n",
    "#     areas = [0.0 for i in range(len(header))]\n",
    "    # check if is empty\n",
    "    if df.detect_label.isnull().values.any():  \n",
    "        return features * 2\n",
    "    # cross levelup features\n",
    "    for i,row in df.iterrows():\n",
    "        for dp in dtct_p:\n",
    "            for cp in clas_p:\n",
    "                if row['classify_label'] in tolerate[row['detect_label']]:\n",
    "                    if row['detect_probability'] > dp and row['classify_probability'] > cp:\n",
    "                        key = \"{}_{:.2f}_{:.3f}\".format(row['detect_label'], dp, cp)\n",
    "                        features[header_map[key]] += 1\n",
    "#                         areas[header_map[key]] += row['w'] * row['h']\n",
    "#     # average areas\n",
    "#     for i in range(len(header)):\n",
    "#         areas[i] /= features[i] if features[i] != 0 else 1.0\n",
    "    \n",
    "    # area balanced numbers\n",
    "    try:\n",
    "        area_mark = 2850000000\n",
    "        area = float(df.area[df.area.notnull()])\n",
    "        features_ab = [f*area_mark/area for f in features]\n",
    "        features += features_ab\n",
    "#         patches = float(df.patches[df.patches.notnull()])\n",
    "#         features_pb = [f*2000/patches for f in features]\n",
    "#         features += features_pb\n",
    "    except:\n",
    "        print(f)\n",
    "        features *= 2\n",
    "\n",
    "#     features += areas\n",
    "    return features\n",
    "\n",
    "def collect(data, test=True):\n",
    "    X = []\n",
    "    ya = []  # all labels\n",
    "    yb = []  # binary labels\n",
    "    names = []\n",
    "    for f in data:\n",
    "        features = extract_oldfashion(f)\n",
    "        if not test:\n",
    "            basename = os.path.splitext(os.path.basename(f))[0]\n",
    "            ls = label[basename].split('+')\n",
    "        else:\n",
    "            basename = os.path.basename(f).split('_BATCH')[0]\n",
    "            if not basename in label_map:\n",
    "                continue\n",
    "            ls = label_map[basename]\n",
    "        if sum(features) == 0:\n",
    "            continue\n",
    "        for l in ls:\n",
    "            a = all_labels[l]\n",
    "            b = bin_labels[l]\n",
    "            X.append(features)\n",
    "            ya.append(a)\n",
    "            yb.append(b)\n",
    "            names.append(f)\n",
    "    return X, ya, yb, names\n",
    "\n",
    "def worker():\n",
    "    files = data\n",
    "    random.shuffle(files)\n",
    "    random.shuffle(files)\n",
    "    print(\"# files:\", len(files))\n",
    "\n",
    "    X, ya, yb, names = [], [], [], []\n",
    "    \n",
    "    executor = ProcessPoolExecutor(max_workers=36)\n",
    "    tasks = []\n",
    "\n",
    "    batch_size = 50\n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch = files[i : i+batch_size]\n",
    "#         collect(batch)\n",
    "        tasks.append(executor.submit(collect, batch))\n",
    "\n",
    "    job_count = len(tasks)\n",
    "    for future in as_completed(tasks):\n",
    "        X_, ya_, yb_, names_ = future.result()  # get the returning result from calling fuction\n",
    "        X += X_\n",
    "        ya += ya_\n",
    "        yb += yb_\n",
    "        names += names_\n",
    "        job_count -= 1\n",
    "        if job_count % 8 == 0: \n",
    "            print(\"One Job Done, Remaining Job Count: %s\" % (job_count))\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    ya = np.asarray(ya)\n",
    "    yb = np.asarray(yb)\n",
    "    print(X.shape, ya.shape, yb.shape)\n",
    "    \n",
    "    return X, ya, yb, names\n",
    "\n",
    "X, ya, yb, names = worker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1407, 2340) (1407,) (1407,)\n",
      "(1036, 2340) (1036,) (1036,)\n"
     ]
    }
   ],
   "source": [
    "# with open('gnet2test1500-2.pkl', 'wb') as f:\n",
    "#     pickle.dump(X, f)\n",
    "#     pickle.dump(ya, f)\n",
    "#     pickle.dump(yb, f)\n",
    "#     pickle.dump(names, f)\n",
    "    \n",
    "with open('./gnet2test1500.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "    ya = pickle.load(f)\n",
    "    yb = pickle.load(f)\n",
    "    names = pickle.load(f)\n",
    "print(X.shape, ya.shape, yb.shape)\n",
    "    \n",
    "# test designated 1000 test data\n",
    "pd_t = pd.read_excel('../test1000.xlsx')\n",
    "nlist = set(pd_t.case_no.values)\n",
    "X_, ya_, yb_, names_ = [], [], [], []\n",
    "for xx, yya, yyb, nn in zip(X, ya, yb, names):\n",
    "    basename = os.path.basename(nn).split('_BATCH')[0]\n",
    "    if not basename in nlist:\n",
    "        continue\n",
    "    X_.append(xx)\n",
    "    ya_.append(yya)\n",
    "    yb_.append(yyb)\n",
    "    names_.append(nn)\n",
    "X = np.asarray(X_)\n",
    "ya = np.asarray(ya_)\n",
    "yb = np.asarray(yb_)\n",
    "names = names_\n",
    "print(X.shape, ya.shape, yb.shape)\n",
    "\n",
    "# # load augmented train data\n",
    "# with open('/home/ssd_array0/Develop/liyu/codect/set1/feature_dict.pkl', 'rb') as f:\n",
    "#     feature_dict = pickle.load(f)\n",
    "    \n",
    "# X, ya, yb = [], [], []\n",
    "# for key,value in feature_dict.items():\n",
    "#     ya += [all_labels[key]] * len(value)\n",
    "#     yb += [0 if key == 'NILM' else 1] * len(value)\n",
    "#     X += value\n",
    "# X = np.asarray(X)\n",
    "# ya = np.asarray(ya)\n",
    "# yb = np.asarray(yb)\n",
    "# print(X.shape, ya.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RFESVM:\n",
    "    def __init__(self):\n",
    "        self.estimator = SVR(kernel=\"linear\")\n",
    "        self.selector = None\n",
    "        \n",
    "    def select(self, X, y, num_feature):\n",
    "        self.selector = RFE(self.estimator, num_feature, step=1)\n",
    "        self.selector = self.selector.fit(X, y)\n",
    "        selected_feature_indices = self.selector.support_ # ndarray of True/False\n",
    "        return selected_feature_indices\n",
    "\n",
    "def split(X, y, mode, test_size, seed):\n",
    "    random.seed(seed)\n",
    "    N = 2 if mode == \"bin\" else 13\n",
    "    idx = {i:[] for i in range(N)}\n",
    "    for i,c in enumerate(y):\n",
    "        idx[c].append(i)\n",
    "    idx_t, idx_v = [], []\n",
    "    for c,indices in idx.items():\n",
    "        n = len(indices)\n",
    "        idx_t += indices[:-int(n*test_size)]\n",
    "        idx_v += indices[-int(n*test_size):]\n",
    "    X_train = X[idx_t]\n",
    "    X_valid = X[idx_v]\n",
    "    y_train = y[idx_t]\n",
    "    y_valid = y[idx_v]\n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "def rfe(X_train, X_valid, y_train, y_valid, num_features):\n",
    "    rfe_svm = RFESVM()\n",
    "    selected_feature_indices = rfe_svm.select(X_train, y_train, num_features)\n",
    "    X_train = X_train[:, selected_feature_indices] # Select elements of numpy array via boolean mask array\n",
    "    X_valid = X_valid[:, selected_feature_indices]\n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "def evaluate(y_valid, y_pred, mode):\n",
    "    if mode == \"bin\":\n",
    "        cur_labels = {0:'NILM', 1:'ABN'}\n",
    "    else:\n",
    "        cur_labels = {value:key for key,value in all_labels.items()}\n",
    "    \n",
    "    results = {i:[0,0,0] for i in range(len(cur_labels))}  # tp, fn, fp\n",
    "    for t,p in zip(y_valid, y_pred):\n",
    "        if t == p:\n",
    "            results[t][0] += 1\n",
    "        else:\n",
    "            results[t][1] += 1\n",
    "            results[p][2] += 1\n",
    "    for i in range(len(results)):\n",
    "        labeli = cur_labels[i]\n",
    "        recall = results[i][0] / (results[i][0] + results[i][1]) if results[i][0] + results[i][1] != 0 else 0.0\n",
    "        precision = results[i][0] / (results[i][0] + results[i][2]) if results[i][0] + results[i][2] != 0 else 0.0\n",
    "        print(labeli, results[i][0] + results[i][1], ' recall = {:.4f}'.format(recall), 'precision = {:.4f}'.format(precision))\n",
    "        \n",
    "    \n",
    "def classify(mode=\"bin\"):  # mode = \"bin\" or \"all\"\n",
    "    seed = 2019\n",
    "    test_size = 0.2\n",
    "#     num_features = 1170\n",
    "\n",
    "    y = yb if mode == \"bin\" else ya\n",
    "#     X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "    X_train, X_valid, y_train, y_valid = split(X, y, mode, test_size, seed)\n",
    "    # X_train, X_valid, y_train, y_valid = rfe(X_train, X_valid, y_train, y_valid, num_features)\n",
    "\n",
    "    model = XGBClassifier(max_depth=15, \n",
    "                          n_jobs=36, \n",
    "                          subsample=0.8, \n",
    "                          colsample_bylevel=1,\n",
    "                          colsample_bytree=0.6, \n",
    "                          scale_pos_weight=1, \n",
    "                          n_estimators=500, \n",
    "                          min_child_weight=1, \n",
    "                          learning_rate=0.1, \n",
    "                          gamma=0,\n",
    "                          random_state=seed)\n",
    "    eval_set = [(X_train, y_train), (X_valid, y_valid)]\n",
    "    if mode == \"bin\":\n",
    "        eval_metric = [\"auc\", \"error\"]\n",
    "    else:\n",
    "        eval_metric = [\"merror\"]\n",
    "    model.fit(X_train, y_train, \n",
    "              early_stopping_rounds=50, \n",
    "              eval_metric=eval_metric, \n",
    "              eval_set=eval_set, \n",
    "              verbose=True)\n",
    "\n",
    "    y_pred = model.predict(X_valid)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    print(\"accuracy: {:.4f}\".format(accuracy))\n",
    "    evaluate(y_valid, y_pred, mode)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# print('binary classification')\n",
    "# bin_model = classify(\"bin\")\n",
    "# print()\n",
    "# print('multilabel classification')\n",
    "# all_model = classify(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_imap = {0:'NILM', 1:'ABN'}\n",
    "all_imap = {0: 'ACTINO', 1: 'AGC', 2: 'ASCH', 3: 'ASCUS', 4: 'CC', 5: 'EC', 6: 'CANDIDA', 7: 'HSIL', 8: 'HSV', 9: 'LSIL', 10: 'NILM', 11: 'SCC', 12: 'TRI'}\n",
    "\n",
    "# with open(\"aug1500.pkl\", 'wb') as f:\n",
    "#     pickle.dump(bin_model, f)\n",
    "#     pickle.dump(all_model, f)\n",
    "#     pickle.dump(bin_imap, f)\n",
    "#     pickle.dump(all_imap, f)\n",
    "\n",
    "\n",
    "# with open(\"train15models.pkl\", 'rb') as f:\n",
    "#     bin_model = pickle.load(f)\n",
    "#     all_model = pickle.load(f)\n",
    "#     bin_imap = pickle.load(f)\n",
    "#     all_imap = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diagnoser:\n",
    "    def __init__(self, pkl_file):\n",
    "        self.bin_cls = range(len(bin_imap))\n",
    "        self.all_cls = range(len(all_imap))\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            self.bin_model = pickle.load(f)\n",
    "            self.all_model = pickle.load(f)\n",
    "            self.bin_imap = pickle.load(f)\n",
    "            self.all_imap = pickle.load(f)\n",
    "    \n",
    "    def extract_old(self, csv_file):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        features = [0 for i in range(len(header))]\n",
    "        # check if is empty\n",
    "        if df.yolo_cell_class.isnull().values.any():  \n",
    "            return features * 2\n",
    "        # cross levelup features\n",
    "        for i,row in df.iterrows():\n",
    "            for dp in dtct_p:\n",
    "                for cp in clas_p:\n",
    "                    if row['xcp_cell_class'] in tolerate[row['yolo_cell_class']]:\n",
    "                        if row['yolo_cell_class_det'] > dp and row['xcp_cell_class_det'] > cp:\n",
    "                            key = \"{}_{:.2f}_{:.3f}\".format(row['yolo_cell_class'], dp, cp)\n",
    "                            features[header_map[key]] += 1\n",
    "        \n",
    "        # area balanced numbers\n",
    "        try:\n",
    "            area_mark = 2850000000\n",
    "            # area = float(df.area[df.area.notnull()])\n",
    "            area = area_map[os.path.basename(f).split('_BATCH')[0]]['area']\n",
    "            features_ab = [f*area_mark/area for f in features]\n",
    "            features += features_ab\n",
    "        except:\n",
    "            print(f)\n",
    "            features *= 2\n",
    "        \n",
    "        features = np.array([features])\n",
    "        return features\n",
    "    \n",
    "    def extract_new(self, csv_file):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        features = [0 for i in range(len(header))]\n",
    "        # check if is empty\n",
    "        if df.detect_label.isnull().values.any():  \n",
    "            return features * 2\n",
    "        # cross levelup features\n",
    "        for i,row in df.iterrows():\n",
    "            for dp in dtct_p:\n",
    "                for cp in clas_p:\n",
    "                    if row['classify_label'] in tolerate[row['detect_label']]:\n",
    "                        if row['detect_probability'] > dp and row['classify_probability'] > cp:\n",
    "                            key = \"{}_{:.2f}_{:.3f}\".format(row['detect_label'], dp, cp)\n",
    "                            features[header_map[key]] += 1\n",
    "                            \n",
    "        # area balanced numbers\n",
    "        try:\n",
    "            area_mark = 2850000000\n",
    "            area = float(df.area[df.area.notnull()])\n",
    "            features_ab = [f*area_mark/area for f in features]\n",
    "            features += features_ab\n",
    "        except:\n",
    "            print(f)\n",
    "            features *= 2\n",
    "                            \n",
    "        features = np.array([features])\n",
    "        return features\n",
    "    \n",
    "#     def bin_predict(self, csv_file):\n",
    "#         f = self.extract_old(csv_file)\n",
    "#         p = self.bin_model.predict(f)[0]\n",
    "#         l = self.bin_imap[p]\n",
    "#         return l\n",
    "        \n",
    "#     def all_predict(self, csv_file):\n",
    "#         f = self.extract_old(csv_file)\n",
    "#         p = self.all_model.predict(f)[0]\n",
    "#         l = self.all_imap[p]\n",
    "#         return l\n",
    "\n",
    "#     def bin_and_all_predict(self, csv_file):\n",
    "#         f = self.extract_old(csv_file)\n",
    "#         pb = self.bin_model.predict(f)[0]\n",
    "#         lb = self.bin_imap[pb]\n",
    "#         pa = self.all_model.predict(f)[0]\n",
    "#         la = self.all_imap[pa]\n",
    "#         return lb, la\n",
    "    \n",
    "    def bin_and_all_predict(self, X):\n",
    "        pb = self.bin_model.predict(X)\n",
    "        lb = [self.bin_imap[p] for p in pb]\n",
    "        pa = self.all_model.predict(X)\n",
    "        la = [self.all_imap[p] for p in pa]\n",
    "        return lb, la\n",
    "    \n",
    "    def bin_predict(self, X, y):\n",
    "        y_pred = self.bin_model.predict(X)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        print(\"accuracy: {:.4f}\".format(accuracy))\n",
    "        evaluate(y, y_pred, 'bin')\n",
    "        cm = confusion_matrix(y, y_pred, labels=self.bin_cls)\n",
    "        return cm\n",
    "        \n",
    "    def all_predict(self, X, y):\n",
    "        y_pred = self.all_model.predict(X)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        print(\"accuracy: {:.4f}\".format(accuracy))\n",
    "        evaluate(y, y_pred, 'all')\n",
    "        cm = confusion_matrix(y, y_pred, labels=self.all_cls)\n",
    "        return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8349\n",
      "NILM 908  recall = 0.8932 precision = 0.9164\n",
      "ABN 128  recall = 0.4219 precision = 0.3576\n",
      "accuracy: 0.8668\n",
      "ACTINO 0  recall = 0.0000 precision = 0.0000\n",
      "AGC 0  recall = 0.0000 precision = 0.0000\n",
      "ASCH 7  recall = 0.0000 precision = 0.0000\n",
      "ASCUS 58  recall = 0.1379 precision = 0.3478\n",
      "CC 18  recall = 0.0556 precision = 1.0000\n",
      "EC 2  recall = 0.0000 precision = 0.0000\n",
      "CANDIDA 22  recall = 0.3182 precision = 0.7000\n",
      "HSIL 2  recall = 1.0000 precision = 0.2500\n",
      "HSV 0  recall = 0.0000 precision = 0.0000\n",
      "LSIL 7  recall = 0.7143 precision = 0.5000\n",
      "NILM 908  recall = 0.9559 precision = 0.9099\n",
      "SCC 1  recall = 1.0000 precision = 1.0000\n",
      "TRI 11  recall = 0.5455 precision = 0.6667\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_no</th>\n",
       "      <th>diagnosis_b</th>\n",
       "      <th>diagnosis_m</th>\n",
       "      <th>label_b</th>\n",
       "      <th>label_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TC19005286</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TC19014812</td>\n",
       "      <td>ABN</td>\n",
       "      <td>NILM</td>\n",
       "      <td>ABN</td>\n",
       "      <td>TRI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TC19012827</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TC19011570</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TC19012865</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TC19014659</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TC19010344</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TC19014702</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TC19010378</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TC19014778</td>\n",
       "      <td>ABN</td>\n",
       "      <td>LSIL</td>\n",
       "      <td>ABN</td>\n",
       "      <td>ASCUS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      case_no diagnosis_b diagnosis_m label_b label_m\n",
       "0  TC19005286        NILM        NILM    NILM    NILM\n",
       "1  TC19014812         ABN        NILM     ABN     TRI\n",
       "2  TC19012827        NILM        NILM    NILM    NILM\n",
       "3  TC19011570        NILM        NILM    NILM    NILM\n",
       "4  TC19012865        NILM        NILM    NILM    NILM\n",
       "5  TC19014659        NILM        NILM    NILM    NILM\n",
       "6  TC19010344        NILM        NILM    NILM    NILM\n",
       "7  TC19014702        NILM        NILM    NILM    NILM\n",
       "8  TC19010378        NILM        NILM    NILM    NILM\n",
       "9  TC19014778         ABN        LSIL     ABN   ASCUS"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_file = \"aug1500.pkl\"\n",
    "d = Diagnoser(pkl_file)\n",
    "\n",
    "b_cm = d.bin_predict(X, yb)\n",
    "a_cm = d.all_predict(X, ya)\n",
    "\n",
    "data_p = {'case_no':[], 'diagnosis_b':[], 'diagnosis_m':[], 'label_b':[], 'label_m':[]}\n",
    "lb, la = d.bin_and_all_predict(X)\n",
    "data_p['case_no'] = [os.path.basename(n).split('_BATCH')[0] for n in names]\n",
    "data_p['diagnosis_b'] = lb\n",
    "data_p['diagnosis_m'] = la\n",
    "data_p['label_b'] = [bin_imap[i] for i in yb]\n",
    "data_p['label_m'] = [all_imap[i] for i in ya]\n",
    "\n",
    "df_p = pd.DataFrame(data=data_p)\n",
    "df_p.to_csv('aug1500p.csv')\n",
    "df_p.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_cm(cm, label_imap, csv_name):\n",
    "    l = len(label_imap)\n",
    "    header = [label_imap[i] for i in range(l)]\n",
    "    with open(csv_name, 'w') as csvf:\n",
    "        writer = csv.writer(csvf, delimiter=',')\n",
    "        writer.writerow(['-']+header)\n",
    "        for i in range(l):\n",
    "            writer.writerow([header[i]] + list(cm[i,:]))\n",
    "    \n",
    "write_cm(b_cm, bin_imap, 'cm-aug1500bin.csv')\n",
    "write_cm(a_cm, all_imap, 'cm-aug1500all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9559471365638766 0.671875 0.04405286343612335 0.8783783783783784 0.328125\n"
     ]
    }
   ],
   "source": [
    "TP = len(df_p[(df_p.diagnosis_m != 'NILM') & (df_p.label_m != 'NILM')])\n",
    "FN = len(df_p[(df_p.diagnosis_m == 'NILM') & (df_p.label_m != 'NILM')])\n",
    "FP = len(df_p[(df_p.diagnosis_m != 'NILM') & (df_p.label_m == 'NILM')])\n",
    "TN = len(df_p[(df_p.diagnosis_m == 'NILM') & (df_p.label_m == 'NILM')])\n",
    "\n",
    "paiyin = TN / (TN + FP)\n",
    "jiayin = FN / (TP + FN)\n",
    "jiayang = FP / (TN + FP)\n",
    "accuracy = (TN + TP) / (TP + FN + FP + TN)\n",
    "sensitivity = TP / (TP + FN)\n",
    "print(paiyin, jiayin, jiayang, accuracy, sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nono = df_p[df_p.diagnosis_m != df_p.label_m]\n",
    "nono.case_no.to_csv('nono2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(mode=\"bin\"):  # mode = \"bin\" or \"all\"\n",
    "    seed = 2018\n",
    "    \n",
    "    y = yb if mode == \"bin\" else ya\n",
    "    num_folds = 5\n",
    "    X_train_folds = np.array_split(X, num_folds)\n",
    "    y_train_folds = np.array_split(y, num_folds)\n",
    "    \n",
    "    best = [0.0, None] # accuracy, i\n",
    "    for i in range(0, num_folds):\n",
    "        X_train = np.concatenate(X_train_folds[:i] + X_train_folds[i+1:])\n",
    "        y_train = np.concatenate(y_train_folds[:i] + y_train_folds[i+1:])\n",
    "        X_valid = X_train_folds[i]\n",
    "        y_valid = y_train_folds[i]\n",
    "\n",
    "        model = XGBClassifier(max_depth=15, \n",
    "                              n_jobs=24, \n",
    "                              subsample=0.8, \n",
    "                              colsample_bylevel=1,\n",
    "                              colsample_bytree=0.6, \n",
    "                              scale_pos_weight=1, \n",
    "                              n_estimators=500, \n",
    "                              min_child_weight=1, \n",
    "                              learning_rate=0.1, \n",
    "                              gamma=0,\n",
    "                              random_state=seed)\n",
    "        eval_set = [(X_train, y_train), (X_valid, y_valid)]\n",
    "        if mode == \"bin\":\n",
    "            eval_metric = [\"auc\", \"error\"]\n",
    "        else:\n",
    "            eval_metric = [\"merror\"]\n",
    "        model.fit(X_train, y_train, \n",
    "                  early_stopping_rounds=50, \n",
    "                  eval_metric=eval_metric, \n",
    "                  eval_set=eval_set, \n",
    "                  verbose=False)\n",
    "\n",
    "        y_pred = model.predict(X_valid)\n",
    "        accuracy = accuracy_score(y_valid, y_pred)\n",
    "        print(\"accuracy: {:.4f}\".format(accuracy))\n",
    "        \n",
    "        if accuracy > best[0]:\n",
    "            best = [accuracy, i]\n",
    "            \n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('binary classification')\n",
    "classify(\"bin\")\n",
    "print()\n",
    "print('multilabel classification')\n",
    "classify(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
