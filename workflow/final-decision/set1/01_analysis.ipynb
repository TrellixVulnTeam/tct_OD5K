{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from multiprocessing import cpu_count\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR, SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train data: map file paths with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"label_train_test.pkl\"\n",
    "with open(a, 'rb') as f:\n",
    "    label, train, test = pickle.load(f)\n",
    "print(len(train), len(test))\n",
    "\n",
    "data_path = './train15/data2800'\n",
    "all_data = os.listdir(data_path)\n",
    "print('# files', len(all_data))\n",
    "data = []\n",
    "all_labels = set()\n",
    "for file_ in all_data:\n",
    "    basename = file_\n",
    "    name = os.path.splitext(basename)[0]\n",
    "    data.append(os.path.join(data_path, basename))\n",
    "    label_ = label[name]\n",
    "    ls = label_.split('+')\n",
    "    for l in ls:\n",
    "        all_labels.add(l)\n",
    "    # print(name, label_)\n",
    "print(len(all_labels), all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_file = './roi_results.txt'\n",
    "area_map = {}\n",
    "with open(area_file, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        name, patches, area = line.strip().split()\n",
    "        name = os.path.splitext(name)[0]\n",
    "        patches = int(patches)\n",
    "        area = float(area)\n",
    "        area_map[name] = {'patches':patches, 'area':area}\n",
    "print(area_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file = './test1521.xlsx'\n",
    "df_l = pd.read_excel(label_file)\n",
    "df_l.head(10)\n",
    "\n",
    "label_map = {}\n",
    "for i,row in df_l.iterrows():\n",
    "    label_map[row['case_no']] = row['old_label'].split('+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './train15/test1521'\n",
    "data = [os.path.join(data_path, f) for f in os.listdir(data_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = data[5]\n",
    "df = pd.read_csv(f)\n",
    "# area = float(df.area[df.area.notnull()])\n",
    "area = area_map[os.path.basename(f).split('_BATCH')[0]]['area']\n",
    "print(area)\n",
    "# patches = float(df.patches[df.patches.notnull()])\n",
    "# print(patches)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ASCUS': 3, 'HSIL': 7, 'SCC': 11, 'CC': 4, 'VIRUS': 8, 'NILM': 10, 'FUNGI': 6, 'ACTINO': 0, 'TRI': 12, 'HSV': 8, 'ASCH': 2, 'LSIL': 9, 'AGC': 1, 'CANDIDA': 6, 'EC': 5}\n",
      "{'ASCUS': 1, 'HSIL': 1, 'SCC': 1, 'VIRUS': 1, 'CC': 1, 'NILM': 0, 'ACTINO': 1, 'ASCH': 1, 'HSV': 1, 'CANDIDA': 1, 'TRI': 1, 'LSIL': 1, 'AGC': 1, 'FUNGI': 1, 'EC': 1}\n"
     ]
    }
   ],
   "source": [
    "tolerate = {\"AGC\":{\"AGC_A\", \"AGC_B\"}, \n",
    "            \"LSIL\":{\"ASCUS\", \"LSIL_E\", \"LSIL_F\"}, \n",
    "            \"ASCUS\":{\"ASCUS\", \"LSIL_E\", \"LSIL_F\"}, \n",
    "            \"HSIL-SCC_G\":{\"HSIL_B\", \"HSIL_M\", \"HSIL_S\", \"SCC_G\"}, \n",
    "            \"SCC_R\":{\"SCC_R\"}, \n",
    "            \"EC\":{\"EC\"}, \n",
    "            \"CC\":{\"CC\"}, \n",
    "            \"VIRUS\":{\"VIRUS\", \"HSV\"}, \n",
    "            \"FUNGI\":{\"FUNGI\", \"CANDIDA\"}, \n",
    "            \"ACTINO\":{\"ACTINO\"}, \n",
    "            \"TRI\":{\"TRI\"}, \n",
    "            \"PH\":{\"PH\"}, \n",
    "            \"SC\":{\"SC\", \"RC\", \"MC\", \"GEC\"}}\n",
    "\n",
    "dtct_p = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "clas_p = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99, 0.999]\n",
    "header = [\"{}_{:.2f}_{:.3f}\".format(key, dp, cp) for key in tolerate for dp in dtct_p for cp in clas_p]\n",
    "header.sort()\n",
    "header_map = {key:i for i,key in enumerate(header)}\n",
    "\n",
    "all_labels = {'ACTINO':0, 'AGC':1, 'ASCH':2, 'ASCUS':3, 'CC':4, 'EC':5, \n",
    "              'FUNGI':6, 'CANDIDA':6, 'HSIL':7, 'VIRUS':8, 'HSV':8, \n",
    "              'LSIL':9, 'NILM':10, 'SCC':11, 'TRI':12}\n",
    "bin_labels = {}\n",
    "for l in all_labels:\n",
    "    if l == 'NILM':\n",
    "        bin_labels[l] = 0\n",
    "    else:\n",
    "        bin_labels[l] = 1\n",
    "print(all_labels)\n",
    "print(bin_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header_imap = {(key, dp, cp):header_map[\"{}_{:.2f}_{:.3f}\".format(key, dp, cp)] for key in tolerate for dp in dtct_p for cp in clas_p}\n",
    "# print(header_imap)\n",
    "\n",
    "# with open('header_imap.pkl', 'wb') as f:\n",
    "#     pickle.dump(header_imap, f)\n",
    "    \n",
    "# # # read pkl file\n",
    "# # with open('header_imap.pkl', 'rb') as f:\n",
    "# #     header_imap = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_oldfashion(f):\n",
    "    df = pd.read_csv(f)\n",
    "    features = [0 for i in range(len(header))]\n",
    "    # check if is empty\n",
    "    if df.yolo_cell_class.isnull().values.any():  \n",
    "        return features * 2\n",
    "    # cross levelup features\n",
    "    for i,row in df.iterrows():\n",
    "        for dp in dtct_p:\n",
    "            for cp in clas_p:\n",
    "                if row['xcp_cell_class'] in tolerate[row['yolo_cell_class']]:\n",
    "                    if row['yolo_cell_class_det'] > dp and row['xcp_cell_class_det'] > cp:\n",
    "                        key = \"{}_{:.2f}_{:.3f}\".format(row['yolo_cell_class'], dp, cp)\n",
    "                        features[header_map[key]] += 1\n",
    "                        \n",
    "    # area balanced numbers\n",
    "    try:\n",
    "        area_mark = 2850000000\n",
    "        # area = float(df.area[df.area.notnull()])\n",
    "        area = area_map[os.path.basename(f).split('_BATCH')[0]]['area']\n",
    "        features_ab = [f*area_mark/area for f in features]\n",
    "        features += features_ab\n",
    "    except:\n",
    "        print(f)\n",
    "        features *= 2\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract(f):\n",
    "    df = pd.read_csv(f)\n",
    "    features = [0 for i in range(len(header))]\n",
    "#     areas = [0.0 for i in range(len(header))]\n",
    "    # check if is empty\n",
    "    if df.detect_label.isnull().values.any():  \n",
    "        return features * 2\n",
    "    # cross levelup features\n",
    "    for i,row in df.iterrows():\n",
    "        for dp in dtct_p:\n",
    "            for cp in clas_p:\n",
    "                if row['classify_label'] in tolerate[row['detect_label']]:\n",
    "                    if row['detect_probability'] > dp and row['classify_probability'] > cp:\n",
    "                        key = \"{}_{:.2f}_{:.3f}\".format(row['detect_label'], dp, cp)\n",
    "                        features[header_map[key]] += 1\n",
    "#                         areas[header_map[key]] += row['w'] * row['h']\n",
    "#     # average areas\n",
    "#     for i in range(len(header)):\n",
    "#         areas[i] /= features[i] if features[i] != 0 else 1.0\n",
    "    \n",
    "    # area balanced numbers\n",
    "    try:\n",
    "        area_mark = 2850000000\n",
    "        area = float(df.area[df.area.notnull()])\n",
    "        features_ab = [f*area_mark/area for f in features]\n",
    "        features += features_ab\n",
    "#         patches = float(df.patches[df.patches.notnull()])\n",
    "#         features_pb = [f*2000/patches for f in features]\n",
    "#         features += features_pb\n",
    "    except:\n",
    "        print(f)\n",
    "        features *= 2\n",
    "\n",
    "#     features += areas\n",
    "    return features\n",
    "\n",
    "def collect(data, test=True):\n",
    "    X = []\n",
    "    ya = []  # all labels\n",
    "    yb = []  # binary labels\n",
    "    names = []\n",
    "    for f in data:\n",
    "        features = extract_oldfashion(f)\n",
    "        if not test:\n",
    "            basename = os.path.splitext(os.path.basename(f))[0]\n",
    "            ls = label[basename].split('+')\n",
    "        else:\n",
    "            basename = os.path.basename(f).split('_BATCH')[0]\n",
    "            if not basename in label_map:\n",
    "                continue\n",
    "            ls = label_map[basename]\n",
    "        if sum(features) == 0:\n",
    "            continue\n",
    "        for l in ls:\n",
    "            a = all_labels[l]\n",
    "            b = bin_labels[l]\n",
    "            X.append(features)\n",
    "            ya.append(a)\n",
    "            yb.append(b)\n",
    "            names.append(f)\n",
    "    return X, ya, yb, names\n",
    "\n",
    "def worker():\n",
    "    files = data\n",
    "    random.shuffle(files)\n",
    "    random.shuffle(files)\n",
    "    print(\"# files:\", len(files))\n",
    "\n",
    "    X, ya, yb, names = [], [], [], []\n",
    "    \n",
    "    executor = ProcessPoolExecutor(max_workers=36)\n",
    "    tasks = []\n",
    "\n",
    "    batch_size = 50\n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch = files[i : i+batch_size]\n",
    "#         collect(batch)\n",
    "        tasks.append(executor.submit(collect, batch))\n",
    "\n",
    "    job_count = len(tasks)\n",
    "    for future in as_completed(tasks):\n",
    "        X_, ya_, yb_, names_ = future.result()  # get the returning result from calling fuction\n",
    "        X += X_\n",
    "        ya += ya_\n",
    "        yb += yb_\n",
    "        names += names_\n",
    "        job_count -= 1\n",
    "        if job_count % 8 == 0: \n",
    "            print(\"One Job Done, Remaining Job Count: %s\" % (job_count))\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    ya = np.asarray(ya)\n",
    "    yb = np.asarray(yb)\n",
    "    print(X.shape, ya.shape, yb.shape)\n",
    "    \n",
    "    return X, ya, yb, names\n",
    "\n",
    "X, ya, yb, names = worker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1420, 2340) (1420,) (1420,)\n",
      "(1045, 2340) (1045,) (1045,)\n"
     ]
    }
   ],
   "source": [
    "# with open('train15test1500.pkl', 'wb') as f:\n",
    "#     pickle.dump(X, f)\n",
    "#     pickle.dump(ya, f)\n",
    "#     pickle.dump(yb, f)\n",
    "#     pickle.dump(names, f)\n",
    "    \n",
    "with open('./train15test1500.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "    ya = pickle.load(f)\n",
    "    yb = pickle.load(f)\n",
    "    names = pickle.load(f)\n",
    "print(X.shape, ya.shape, yb.shape)\n",
    "    \n",
    "# test designated 1000 test data\n",
    "pd_t = pd.read_excel('./test1000.xlsx')\n",
    "nlist = set(pd_t.case_no.values)\n",
    "X_, ya_, yb_, names_ = [], [], [], []\n",
    "for xx, yya, yyb, nn in zip(X, ya, yb, names):\n",
    "    basename = os.path.basename(nn).split('_BATCH')[0]\n",
    "    if not basename in nlist:\n",
    "        continue\n",
    "    X_.append(xx)\n",
    "    ya_.append(yya)\n",
    "    yb_.append(yyb)\n",
    "    names_.append(nn)\n",
    "X = np.asarray(X_)\n",
    "ya = np.asarray(ya_)\n",
    "yb = np.asarray(yb_)\n",
    "names = names_\n",
    "print(X.shape, ya.shape, yb.shape)\n",
    "\n",
    "# # load augmented train data\n",
    "# with open('/home/ssd_array0/Develop/liyu/codect/set1/feature_dict.pkl', 'rb') as f:\n",
    "#     feature_dict = pickle.load(f)\n",
    "    \n",
    "# X, ya, yb = [], [], []\n",
    "# for key,value in feature_dict.items():\n",
    "#     ya += [all_labels[key]] * len(value)\n",
    "#     yb += [0 if key == 'NILM' else 1] * len(value)\n",
    "#     X += value\n",
    "# X = np.asarray(X)\n",
    "# ya = np.asarray(ya)\n",
    "# yb = np.asarray(yb)\n",
    "# print(X.shape, ya.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary classification\n",
      "[0]\tvalidation_0-auc:0.930266\tvalidation_0-error:0.022692\tvalidation_1-auc:0.906534\tvalidation_1-error:0.21641\n",
      "Multiple eval metrics have been passed: 'validation_1-error' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-error hasn't improved in 50 rounds.\n",
      "[1]\tvalidation_0-auc:0.957291\tvalidation_0-error:0.023077\tvalidation_1-auc:0.93054\tvalidation_1-error:0.150769\n",
      "[2]\tvalidation_0-auc:0.971692\tvalidation_0-error:0.019295\tvalidation_1-auc:0.933187\tvalidation_1-error:0.192051\n",
      "[3]\tvalidation_0-auc:0.982395\tvalidation_0-error:0.019231\tvalidation_1-auc:0.942594\tvalidation_1-error:0.181026\n",
      "[4]\tvalidation_0-auc:0.983084\tvalidation_0-error:0.019103\tvalidation_1-auc:0.940202\tvalidation_1-error:0.167949\n",
      "[5]\tvalidation_0-auc:0.984584\tvalidation_0-error:0.018205\tvalidation_1-auc:0.944289\tvalidation_1-error:0.168205\n",
      "[6]\tvalidation_0-auc:0.994121\tvalidation_0-error:0.017756\tvalidation_1-auc:0.948122\tvalidation_1-error:0.167692\n",
      "[7]\tvalidation_0-auc:0.995584\tvalidation_0-error:0.016282\tvalidation_1-auc:0.953855\tvalidation_1-error:0.16641\n",
      "[8]\tvalidation_0-auc:0.995654\tvalidation_0-error:0.015513\tvalidation_1-auc:0.955926\tvalidation_1-error:0.16641\n",
      "[9]\tvalidation_0-auc:0.9968\tvalidation_0-error:0.015577\tvalidation_1-auc:0.959733\tvalidation_1-error:0.167692\n",
      "[10]\tvalidation_0-auc:0.997118\tvalidation_0-error:0.014231\tvalidation_1-auc:0.962865\tvalidation_1-error:0.173846\n",
      "[11]\tvalidation_0-auc:0.997513\tvalidation_0-error:0.013654\tvalidation_1-auc:0.961976\tvalidation_1-error:0.180769\n",
      "[12]\tvalidation_0-auc:0.998472\tvalidation_0-error:0.012372\tvalidation_1-auc:0.961503\tvalidation_1-error:0.192308\n",
      "[13]\tvalidation_0-auc:0.998771\tvalidation_0-error:0.011987\tvalidation_1-auc:0.962957\tvalidation_1-error:0.191538\n",
      "[14]\tvalidation_0-auc:0.998868\tvalidation_0-error:0.011346\tvalidation_1-auc:0.965692\tvalidation_1-error:0.192051\n",
      "[15]\tvalidation_0-auc:0.998999\tvalidation_0-error:0.010256\tvalidation_1-auc:0.970722\tvalidation_1-error:0.179231\n",
      "[16]\tvalidation_0-auc:0.999045\tvalidation_0-error:0.01\tvalidation_1-auc:0.970952\tvalidation_1-error:0.171282\n",
      "[17]\tvalidation_0-auc:0.999124\tvalidation_0-error:0.009872\tvalidation_1-auc:0.971346\tvalidation_1-error:0.189744\n",
      "[18]\tvalidation_0-auc:0.999195\tvalidation_0-error:0.009167\tvalidation_1-auc:0.971572\tvalidation_1-error:0.188462\n",
      "[19]\tvalidation_0-auc:0.999271\tvalidation_0-error:0.008526\tvalidation_1-auc:0.973462\tvalidation_1-error:0.188205\n",
      "[20]\tvalidation_0-auc:0.999296\tvalidation_0-error:0.008269\tvalidation_1-auc:0.975154\tvalidation_1-error:0.177179\n",
      "[21]\tvalidation_0-auc:0.999362\tvalidation_0-error:0.007564\tvalidation_1-auc:0.976648\tvalidation_1-error:0.15\n",
      "[22]\tvalidation_0-auc:0.999397\tvalidation_0-error:0.007436\tvalidation_1-auc:0.976231\tvalidation_1-error:0.156923\n",
      "[23]\tvalidation_0-auc:0.999402\tvalidation_0-error:0.006923\tvalidation_1-auc:0.976716\tvalidation_1-error:0.175385\n",
      "[24]\tvalidation_0-auc:0.999439\tvalidation_0-error:0.006474\tvalidation_1-auc:0.97657\tvalidation_1-error:0.186154\n",
      "[25]\tvalidation_0-auc:0.999476\tvalidation_0-error:0.00609\tvalidation_1-auc:0.976435\tvalidation_1-error:0.187179\n",
      "[26]\tvalidation_0-auc:0.999521\tvalidation_0-error:0.005897\tvalidation_1-auc:0.978454\tvalidation_1-error:0.176667\n",
      "[27]\tvalidation_0-auc:0.999557\tvalidation_0-error:0.005321\tvalidation_1-auc:0.978784\tvalidation_1-error:0.165385\n",
      "[28]\tvalidation_0-auc:0.999582\tvalidation_0-error:0.005192\tvalidation_1-auc:0.979667\tvalidation_1-error:0.175641\n",
      "[29]\tvalidation_0-auc:0.999594\tvalidation_0-error:0.005064\tvalidation_1-auc:0.980525\tvalidation_1-error:0.165128\n",
      "[30]\tvalidation_0-auc:0.999613\tvalidation_0-error:0.005\tvalidation_1-auc:0.980525\tvalidation_1-error:0.165128\n",
      "[31]\tvalidation_0-auc:0.999639\tvalidation_0-error:0.004679\tvalidation_1-auc:0.981553\tvalidation_1-error:0.165385\n",
      "[32]\tvalidation_0-auc:0.999652\tvalidation_0-error:0.004808\tvalidation_1-auc:0.981596\tvalidation_1-error:0.175641\n",
      "[33]\tvalidation_0-auc:0.999653\tvalidation_0-error:0.004615\tvalidation_1-auc:0.982662\tvalidation_1-error:0.165128\n",
      "[34]\tvalidation_0-auc:0.999667\tvalidation_0-error:0.004423\tvalidation_1-auc:0.983669\tvalidation_1-error:0.164872\n",
      "[35]\tvalidation_0-auc:0.99969\tvalidation_0-error:0.004423\tvalidation_1-auc:0.98339\tvalidation_1-error:0.164872\n",
      "[36]\tvalidation_0-auc:0.999716\tvalidation_0-error:0.004038\tvalidation_1-auc:0.984506\tvalidation_1-error:0.164615\n",
      "[37]\tvalidation_0-auc:0.999755\tvalidation_0-error:0.003782\tvalidation_1-auc:0.98431\tvalidation_1-error:0.164359\n",
      "[38]\tvalidation_0-auc:0.99979\tvalidation_0-error:0.00359\tvalidation_1-auc:0.984987\tvalidation_1-error:0.163846\n",
      "[39]\tvalidation_0-auc:0.999808\tvalidation_0-error:0.003333\tvalidation_1-auc:0.98506\tvalidation_1-error:0.163333\n",
      "[40]\tvalidation_0-auc:0.999818\tvalidation_0-error:0.003077\tvalidation_1-auc:0.984768\tvalidation_1-error:0.163333\n",
      "[41]\tvalidation_0-auc:0.999833\tvalidation_0-error:0.003141\tvalidation_1-auc:0.986349\tvalidation_1-error:0.163333\n",
      "[42]\tvalidation_0-auc:0.999851\tvalidation_0-error:0.002756\tvalidation_1-auc:0.986288\tvalidation_1-error:0.163077\n",
      "[43]\tvalidation_0-auc:0.999856\tvalidation_0-error:0.002756\tvalidation_1-auc:0.987197\tvalidation_1-error:0.163077\n",
      "[44]\tvalidation_0-auc:0.999865\tvalidation_0-error:0.002628\tvalidation_1-auc:0.988735\tvalidation_1-error:0.163077\n",
      "[45]\tvalidation_0-auc:0.999882\tvalidation_0-error:0.002244\tvalidation_1-auc:0.989143\tvalidation_1-error:0.163077\n",
      "[46]\tvalidation_0-auc:0.999895\tvalidation_0-error:0.002244\tvalidation_1-auc:0.989961\tvalidation_1-error:0.163333\n",
      "[47]\tvalidation_0-auc:0.999904\tvalidation_0-error:0.002179\tvalidation_1-auc:0.989978\tvalidation_1-error:0.163846\n",
      "[48]\tvalidation_0-auc:0.999914\tvalidation_0-error:0.002115\tvalidation_1-auc:0.989814\tvalidation_1-error:0.165128\n",
      "[49]\tvalidation_0-auc:0.999923\tvalidation_0-error:0.001795\tvalidation_1-auc:0.989919\tvalidation_1-error:0.164615\n",
      "[50]\tvalidation_0-auc:0.999933\tvalidation_0-error:0.001603\tvalidation_1-auc:0.990041\tvalidation_1-error:0.164359\n",
      "[51]\tvalidation_0-auc:0.99994\tvalidation_0-error:0.001538\tvalidation_1-auc:0.990954\tvalidation_1-error:0.165128\n",
      "[52]\tvalidation_0-auc:0.999944\tvalidation_0-error:0.001282\tvalidation_1-auc:0.991418\tvalidation_1-error:0.164359\n",
      "[53]\tvalidation_0-auc:0.999952\tvalidation_0-error:0.00109\tvalidation_1-auc:0.992047\tvalidation_1-error:0.164359\n",
      "[54]\tvalidation_0-auc:0.999954\tvalidation_0-error:0.001026\tvalidation_1-auc:0.991928\tvalidation_1-error:0.164359\n",
      "[55]\tvalidation_0-auc:0.999958\tvalidation_0-error:0.000897\tvalidation_1-auc:0.992442\tvalidation_1-error:0.164359\n",
      "[56]\tvalidation_0-auc:0.99996\tvalidation_0-error:0.000833\tvalidation_1-auc:0.992104\tvalidation_1-error:0.164359\n",
      "[57]\tvalidation_0-auc:0.999961\tvalidation_0-error:0.000833\tvalidation_1-auc:0.992509\tvalidation_1-error:0.164615\n",
      "[58]\tvalidation_0-auc:0.999965\tvalidation_0-error:0.000833\tvalidation_1-auc:0.992797\tvalidation_1-error:0.164103\n",
      "[59]\tvalidation_0-auc:0.999964\tvalidation_0-error:0.000833\tvalidation_1-auc:0.992955\tvalidation_1-error:0.164103\n",
      "[60]\tvalidation_0-auc:0.999969\tvalidation_0-error:0.000705\tvalidation_1-auc:0.992787\tvalidation_1-error:0.164615\n",
      "[61]\tvalidation_0-auc:0.999972\tvalidation_0-error:0.000705\tvalidation_1-auc:0.993183\tvalidation_1-error:0.164359\n",
      "[62]\tvalidation_0-auc:0.999975\tvalidation_0-error:0.000641\tvalidation_1-auc:0.993305\tvalidation_1-error:0.164359\n",
      "[63]\tvalidation_0-auc:0.999977\tvalidation_0-error:0.000705\tvalidation_1-auc:0.993308\tvalidation_1-error:0.164359\n",
      "[64]\tvalidation_0-auc:0.999979\tvalidation_0-error:0.000641\tvalidation_1-auc:0.994105\tvalidation_1-error:0.164359\n",
      "[65]\tvalidation_0-auc:0.99998\tvalidation_0-error:0.000641\tvalidation_1-auc:0.994662\tvalidation_1-error:0.164615\n",
      "[66]\tvalidation_0-auc:0.999981\tvalidation_0-error:0.000641\tvalidation_1-auc:0.99474\tvalidation_1-error:0.164359\n",
      "[67]\tvalidation_0-auc:0.999984\tvalidation_0-error:0.000641\tvalidation_1-auc:0.994669\tvalidation_1-error:0.164359\n",
      "[68]\tvalidation_0-auc:0.999986\tvalidation_0-error:0.000577\tvalidation_1-auc:0.994314\tvalidation_1-error:0.164615\n",
      "[69]\tvalidation_0-auc:0.999988\tvalidation_0-error:0.000641\tvalidation_1-auc:0.994699\tvalidation_1-error:0.164872\n",
      "[70]\tvalidation_0-auc:0.999989\tvalidation_0-error:0.000577\tvalidation_1-auc:0.994762\tvalidation_1-error:0.164872\n",
      "[71]\tvalidation_0-auc:0.99999\tvalidation_0-error:0.000577\tvalidation_1-auc:0.995148\tvalidation_1-error:0.164872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping. Best iteration:\n",
      "[21]\tvalidation_0-auc:0.999362\tvalidation_0-error:0.007564\tvalidation_1-auc:0.976648\tvalidation_1-error:0.15\n",
      "\n",
      "accuracy: 0.8500\n",
      "NILM 300  recall = 0.9300 precision = 0.3310\n",
      "ABN 3600  recall = 0.8433 precision = 0.9931\n",
      "\n",
      "multilabel classification\n",
      "[0]\tvalidation_0-merror:0.036154\tvalidation_1-merror:0.017436\n",
      "Multiple eval metrics have been passed: 'validation_1-merror' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-merror hasn't improved in 50 rounds.\n",
      "[1]\tvalidation_0-merror:0.028462\tvalidation_1-merror:0.013077\n",
      "[2]\tvalidation_0-merror:0.025705\tvalidation_1-merror:0.011538\n",
      "[3]\tvalidation_0-merror:0.02359\tvalidation_1-merror:0.008205\n",
      "[4]\tvalidation_0-merror:0.022692\tvalidation_1-merror:0.008205\n",
      "[5]\tvalidation_0-merror:0.021795\tvalidation_1-merror:0.008462\n",
      "[6]\tvalidation_0-merror:0.021026\tvalidation_1-merror:0.006923\n",
      "[7]\tvalidation_0-merror:0.019615\tvalidation_1-merror:0.006667\n",
      "[8]\tvalidation_0-merror:0.019038\tvalidation_1-merror:0.006667\n",
      "[9]\tvalidation_0-merror:0.018462\tvalidation_1-merror:0.006667\n",
      "[10]\tvalidation_0-merror:0.018141\tvalidation_1-merror:0.006154\n",
      "[11]\tvalidation_0-merror:0.018141\tvalidation_1-merror:0.004872\n",
      "[12]\tvalidation_0-merror:0.017244\tvalidation_1-merror:0.004359\n",
      "[13]\tvalidation_0-merror:0.016795\tvalidation_1-merror:0.004615\n",
      "[14]\tvalidation_0-merror:0.016538\tvalidation_1-merror:0.004103\n",
      "[15]\tvalidation_0-merror:0.016154\tvalidation_1-merror:0.004103\n",
      "[16]\tvalidation_0-merror:0.015641\tvalidation_1-merror:0.004103\n",
      "[17]\tvalidation_0-merror:0.015\tvalidation_1-merror:0.004103\n",
      "[18]\tvalidation_0-merror:0.014679\tvalidation_1-merror:0.004103\n",
      "[19]\tvalidation_0-merror:0.014231\tvalidation_1-merror:0.003846\n",
      "[20]\tvalidation_0-merror:0.013782\tvalidation_1-merror:0.002821\n",
      "[21]\tvalidation_0-merror:0.013397\tvalidation_1-merror:0.002821\n",
      "[22]\tvalidation_0-merror:0.012949\tvalidation_1-merror:0.002821\n",
      "[23]\tvalidation_0-merror:0.012436\tvalidation_1-merror:0.002821\n",
      "[24]\tvalidation_0-merror:0.012372\tvalidation_1-merror:0.002821\n",
      "[25]\tvalidation_0-merror:0.011859\tvalidation_1-merror:0.002821\n",
      "[26]\tvalidation_0-merror:0.011731\tvalidation_1-merror:0.002821\n",
      "[27]\tvalidation_0-merror:0.011346\tvalidation_1-merror:0.002821\n",
      "[28]\tvalidation_0-merror:0.011026\tvalidation_1-merror:0.002051\n",
      "[29]\tvalidation_0-merror:0.010577\tvalidation_1-merror:0.002051\n",
      "[30]\tvalidation_0-merror:0.010449\tvalidation_1-merror:0.002051\n",
      "[31]\tvalidation_0-merror:0.010128\tvalidation_1-merror:0.002051\n",
      "[32]\tvalidation_0-merror:0.01\tvalidation_1-merror:0.002051\n",
      "[33]\tvalidation_0-merror:0.009744\tvalidation_1-merror:0.002051\n",
      "[34]\tvalidation_0-merror:0.009359\tvalidation_1-merror:0.002051\n",
      "[35]\tvalidation_0-merror:0.009103\tvalidation_1-merror:0.002051\n",
      "[36]\tvalidation_0-merror:0.008718\tvalidation_1-merror:0.001282\n",
      "[37]\tvalidation_0-merror:0.008269\tvalidation_1-merror:0.001282\n",
      "[38]\tvalidation_0-merror:0.008077\tvalidation_1-merror:0.001282\n",
      "[39]\tvalidation_0-merror:0.007885\tvalidation_1-merror:0.001282\n",
      "[40]\tvalidation_0-merror:0.007756\tvalidation_1-merror:0.001282\n",
      "[41]\tvalidation_0-merror:0.007628\tvalidation_1-merror:0.001282\n",
      "[42]\tvalidation_0-merror:0.0075\tvalidation_1-merror:0.001282\n",
      "[43]\tvalidation_0-merror:0.007372\tvalidation_1-merror:0.001026\n",
      "[44]\tvalidation_0-merror:0.007244\tvalidation_1-merror:0.000769\n",
      "[45]\tvalidation_0-merror:0.007115\tvalidation_1-merror:0.000769\n",
      "[46]\tvalidation_0-merror:0.007115\tvalidation_1-merror:0.000769\n",
      "[47]\tvalidation_0-merror:0.006987\tvalidation_1-merror:0.000769\n",
      "[48]\tvalidation_0-merror:0.006795\tvalidation_1-merror:0.000769\n",
      "[49]\tvalidation_0-merror:0.006731\tvalidation_1-merror:0.000769\n",
      "[50]\tvalidation_0-merror:0.006667\tvalidation_1-merror:0.000769\n",
      "[51]\tvalidation_0-merror:0.006603\tvalidation_1-merror:0.000769\n",
      "[52]\tvalidation_0-merror:0.006538\tvalidation_1-merror:0.000769\n",
      "[53]\tvalidation_0-merror:0.006282\tvalidation_1-merror:0.000769\n",
      "[54]\tvalidation_0-merror:0.006282\tvalidation_1-merror:0.000769\n",
      "[55]\tvalidation_0-merror:0.006282\tvalidation_1-merror:0.000769\n",
      "[56]\tvalidation_0-merror:0.006282\tvalidation_1-merror:0.000769\n",
      "[57]\tvalidation_0-merror:0.006218\tvalidation_1-merror:0.000769\n",
      "[58]\tvalidation_0-merror:0.006154\tvalidation_1-merror:0.000769\n",
      "[59]\tvalidation_0-merror:0.006026\tvalidation_1-merror:0.000769\n",
      "[60]\tvalidation_0-merror:0.006026\tvalidation_1-merror:0.000769\n",
      "[61]\tvalidation_0-merror:0.006026\tvalidation_1-merror:0.000769\n",
      "[62]\tvalidation_0-merror:0.006026\tvalidation_1-merror:0.000769\n",
      "[63]\tvalidation_0-merror:0.005897\tvalidation_1-merror:0.000769\n",
      "[64]\tvalidation_0-merror:0.005833\tvalidation_1-merror:0.000769\n",
      "[65]\tvalidation_0-merror:0.005833\tvalidation_1-merror:0.000769\n",
      "[66]\tvalidation_0-merror:0.005833\tvalidation_1-merror:0.000769\n",
      "[67]\tvalidation_0-merror:0.005833\tvalidation_1-merror:0.000769\n",
      "[68]\tvalidation_0-merror:0.005833\tvalidation_1-merror:0.000769\n",
      "[69]\tvalidation_0-merror:0.005833\tvalidation_1-merror:0.000769\n",
      "[70]\tvalidation_0-merror:0.005833\tvalidation_1-merror:0.000769\n",
      "[71]\tvalidation_0-merror:0.005833\tvalidation_1-merror:0.000769\n",
      "[72]\tvalidation_0-merror:0.005833\tvalidation_1-merror:0.000769\n",
      "[73]\tvalidation_0-merror:0.005833\tvalidation_1-merror:0.000769\n",
      "[74]\tvalidation_0-merror:0.005833\tvalidation_1-merror:0.000769\n",
      "[75]\tvalidation_0-merror:0.005769\tvalidation_1-merror:0.000769\n",
      "[76]\tvalidation_0-merror:0.005769\tvalidation_1-merror:0.000769\n",
      "[77]\tvalidation_0-merror:0.005769\tvalidation_1-merror:0.000769\n",
      "[78]\tvalidation_0-merror:0.005769\tvalidation_1-merror:0.000769\n",
      "[79]\tvalidation_0-merror:0.005769\tvalidation_1-merror:0.000769\n",
      "[80]\tvalidation_0-merror:0.005769\tvalidation_1-merror:0.000769\n",
      "[81]\tvalidation_0-merror:0.005769\tvalidation_1-merror:0.000769\n",
      "[82]\tvalidation_0-merror:0.005769\tvalidation_1-merror:0.000769\n",
      "[83]\tvalidation_0-merror:0.005769\tvalidation_1-merror:0.000769\n",
      "[84]\tvalidation_0-merror:0.005705\tvalidation_1-merror:0.000769\n",
      "[85]\tvalidation_0-merror:0.005705\tvalidation_1-merror:0.000769\n",
      "[86]\tvalidation_0-merror:0.005705\tvalidation_1-merror:0.000769\n",
      "[87]\tvalidation_0-merror:0.005641\tvalidation_1-merror:0.000769\n",
      "[88]\tvalidation_0-merror:0.005641\tvalidation_1-merror:0.000769\n",
      "[89]\tvalidation_0-merror:0.005641\tvalidation_1-merror:0.000769\n",
      "[90]\tvalidation_0-merror:0.005641\tvalidation_1-merror:0.000769\n",
      "[91]\tvalidation_0-merror:0.005641\tvalidation_1-merror:0.000769\n",
      "[92]\tvalidation_0-merror:0.005641\tvalidation_1-merror:0.000769\n",
      "[93]\tvalidation_0-merror:0.005641\tvalidation_1-merror:0.000769\n",
      "[94]\tvalidation_0-merror:0.005641\tvalidation_1-merror:0.000769\n",
      "Stopping. Best iteration:\n",
      "[44]\tvalidation_0-merror:0.007244\tvalidation_1-merror:0.000769\n",
      "\n",
      "accuracy: 0.9992\n",
      "ACTINO 300  recall = 1.0000 precision = 1.0000\n",
      "AGC 300  recall = 1.0000 precision = 1.0000\n",
      "ASCH 300  recall = 1.0000 precision = 1.0000\n",
      "ASCUS 300  recall = 1.0000 precision = 1.0000\n",
      "CC 300  recall = 1.0000 precision = 1.0000\n",
      "EC 300  recall = 1.0000 precision = 1.0000\n",
      "CANDIDA 300  recall = 0.9900 precision = 1.0000\n",
      "HSIL 300  recall = 1.0000 precision = 1.0000\n",
      "HSV 300  recall = 1.0000 precision = 1.0000\n",
      "LSIL 300  recall = 1.0000 precision = 0.9901\n",
      "NILM 300  recall = 1.0000 precision = 1.0000\n",
      "SCC 300  recall = 1.0000 precision = 1.0000\n",
      "TRI 300  recall = 1.0000 precision = 1.0000\n"
     ]
    }
   ],
   "source": [
    "class RFESVM:\n",
    "    def __init__(self):\n",
    "        self.estimator = SVR(kernel=\"linear\")\n",
    "        self.selector = None\n",
    "        \n",
    "    def select(self, X, y, num_feature):\n",
    "        self.selector = RFE(self.estimator, num_feature, step=1)\n",
    "        self.selector = self.selector.fit(X, y)\n",
    "        selected_feature_indices = self.selector.support_ # ndarray of True/False\n",
    "        return selected_feature_indices\n",
    "\n",
    "def split(X, y, mode, test_size, seed):\n",
    "    random.seed(seed)\n",
    "    N = 2 if mode == \"bin\" else 13\n",
    "    idx = {i:[] for i in range(N)}\n",
    "    for i,c in enumerate(y):\n",
    "        idx[c].append(i)\n",
    "    idx_t, idx_v = [], []\n",
    "    for c,indices in idx.items():\n",
    "        n = len(indices)\n",
    "        idx_t += indices[:-int(n*test_size)]\n",
    "        idx_v += indices[-int(n*test_size):]\n",
    "    X_train = X[idx_t]\n",
    "    X_valid = X[idx_v]\n",
    "    y_train = y[idx_t]\n",
    "    y_valid = y[idx_v]\n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "def rfe(X_train, X_valid, y_train, y_valid, num_features):\n",
    "    rfe_svm = RFESVM()\n",
    "    selected_feature_indices = rfe_svm.select(X_train, y_train, num_features)\n",
    "    X_train = X_train[:, selected_feature_indices] # Select elements of numpy array via boolean mask array\n",
    "    X_valid = X_valid[:, selected_feature_indices]\n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "def evaluate(y_valid, y_pred, mode):\n",
    "    if mode == \"bin\":\n",
    "        cur_labels = {0:'NILM', 1:'ABN'}\n",
    "    else:\n",
    "        cur_labels = {value:key for key,value in all_labels.items()}\n",
    "    \n",
    "    results = {i:[0,0,0] for i in range(len(cur_labels))}  # tp, fn, fp\n",
    "    for t,p in zip(y_valid, y_pred):\n",
    "        if t == p:\n",
    "            results[t][0] += 1\n",
    "        else:\n",
    "            results[t][1] += 1\n",
    "            results[p][2] += 1\n",
    "    for i in range(len(results)):\n",
    "        labeli = cur_labels[i]\n",
    "        recall = results[i][0] / (results[i][0] + results[i][1]) if results[i][0] + results[i][1] != 0 else 0.0\n",
    "        precision = results[i][0] / (results[i][0] + results[i][2]) if results[i][0] + results[i][2] != 0 else 0.0\n",
    "        print(labeli, results[i][0] + results[i][1], ' recall = {:.4f}'.format(recall), 'precision = {:.4f}'.format(precision))\n",
    "        \n",
    "    \n",
    "def classify(mode=\"bin\"):  # mode = \"bin\" or \"all\"\n",
    "    seed = 2019\n",
    "    test_size = 0.2\n",
    "#     num_features = 1170\n",
    "\n",
    "    y = yb if mode == \"bin\" else ya\n",
    "#     X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "    X_train, X_valid, y_train, y_valid = split(X, y, mode, test_size, seed)\n",
    "    # X_train, X_valid, y_train, y_valid = rfe(X_train, X_valid, y_train, y_valid, num_features)\n",
    "\n",
    "    model = XGBClassifier(max_depth=15, \n",
    "                          n_jobs=36, \n",
    "                          subsample=0.8, \n",
    "                          colsample_bylevel=1,\n",
    "                          colsample_bytree=0.6, \n",
    "                          scale_pos_weight=1, \n",
    "                          n_estimators=500, \n",
    "                          min_child_weight=1, \n",
    "                          learning_rate=0.1, \n",
    "                          gamma=0,\n",
    "                          random_state=seed)\n",
    "    eval_set = [(X_train, y_train), (X_valid, y_valid)]\n",
    "    if mode == \"bin\":\n",
    "        eval_metric = [\"auc\", \"error\"]\n",
    "    else:\n",
    "        eval_metric = [\"merror\"]\n",
    "    model.fit(X_train, y_train, \n",
    "              early_stopping_rounds=50, \n",
    "              eval_metric=eval_metric, \n",
    "              eval_set=eval_set, \n",
    "              verbose=True)\n",
    "\n",
    "    y_pred = model.predict(X_valid)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    print(\"accuracy: {:.4f}\".format(accuracy))\n",
    "    evaluate(y_valid, y_pred, mode)\n",
    "    \n",
    "    return model\n",
    "\n",
    "print('binary classification')\n",
    "bin_model = classify(\"bin\")\n",
    "print()\n",
    "print('multilabel classification')\n",
    "all_model = classify(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_imap = {0:'NILM', 1:'ABN'}\n",
    "all_imap = {0: 'ACTINO', 1: 'AGC', 2: 'ASCH', 3: 'ASCUS', 4: 'CC', 5: 'EC', 6: 'CANDIDA', 7: 'HSIL', 8: 'HSV', 9: 'LSIL', 10: 'NILM', 11: 'SCC', 12: 'TRI'}\n",
    "\n",
    "# with open(\"aug1500.pkl\", 'wb') as f:\n",
    "#     pickle.dump(bin_model, f)\n",
    "#     pickle.dump(all_model, f)\n",
    "#     pickle.dump(bin_imap, f)\n",
    "#     pickle.dump(all_imap, f)\n",
    "\n",
    "\n",
    "# with open(\"train15models.pkl\", 'rb') as f:\n",
    "#     bin_model = pickle.load(f)\n",
    "#     all_model = pickle.load(f)\n",
    "#     bin_imap = pickle.load(f)\n",
    "#     all_imap = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diagnoser:\n",
    "    def __init__(self, pkl_file):\n",
    "        self.bin_cls = range(len(bin_imap))\n",
    "        self.all_cls = range(len(all_imap))\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            self.bin_model = pickle.load(f)\n",
    "            self.all_model = pickle.load(f)\n",
    "            self.bin_imap = pickle.load(f)\n",
    "            self.all_imap = pickle.load(f)\n",
    "    \n",
    "    def extract_old(self, csv_file):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        features = [0 for i in range(len(header))]\n",
    "        # check if is empty\n",
    "        if df.yolo_cell_class.isnull().values.any():  \n",
    "            return features * 2\n",
    "        # cross levelup features\n",
    "        for i,row in df.iterrows():\n",
    "            for dp in dtct_p:\n",
    "                for cp in clas_p:\n",
    "                    if row['xcp_cell_class'] in tolerate[row['yolo_cell_class']]:\n",
    "                        if row['yolo_cell_class_det'] > dp and row['xcp_cell_class_det'] > cp:\n",
    "                            key = \"{}_{:.2f}_{:.3f}\".format(row['yolo_cell_class'], dp, cp)\n",
    "                            features[header_map[key]] += 1\n",
    "        \n",
    "        # area balanced numbers\n",
    "        try:\n",
    "            area_mark = 2850000000\n",
    "            # area = float(df.area[df.area.notnull()])\n",
    "            area = area_map[os.path.basename(f).split('_BATCH')[0]]['area']\n",
    "            features_ab = [f*area_mark/area for f in features]\n",
    "            features += features_ab\n",
    "        except:\n",
    "            print(f)\n",
    "            features *= 2\n",
    "        \n",
    "        features = np.array([features])\n",
    "        return features\n",
    "    \n",
    "    def extract_new(self, csv_file):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        features = [0 for i in range(len(header))]\n",
    "        # check if is empty\n",
    "        if df.detect_label.isnull().values.any():  \n",
    "            return features * 2\n",
    "        # cross levelup features\n",
    "        for i,row in df.iterrows():\n",
    "            for dp in dtct_p:\n",
    "                for cp in clas_p:\n",
    "                    if row['classify_label'] in tolerate[row['detect_label']]:\n",
    "                        if row['detect_probability'] > dp and row['classify_probability'] > cp:\n",
    "                            key = \"{}_{:.2f}_{:.3f}\".format(row['detect_label'], dp, cp)\n",
    "                            features[header_map[key]] += 1\n",
    "                            \n",
    "        # area balanced numbers\n",
    "        try:\n",
    "            area_mark = 2850000000\n",
    "            area = float(df.area[df.area.notnull()])\n",
    "            features_ab = [f*area_mark/area for f in features]\n",
    "            features += features_ab\n",
    "        except:\n",
    "            print(f)\n",
    "            features *= 2\n",
    "                            \n",
    "        features = np.array([features])\n",
    "        return features\n",
    "    \n",
    "#     def bin_predict(self, csv_file):\n",
    "#         f = self.extract_old(csv_file)\n",
    "#         p = self.bin_model.predict(f)[0]\n",
    "#         l = self.bin_imap[p]\n",
    "#         return l\n",
    "        \n",
    "#     def all_predict(self, csv_file):\n",
    "#         f = self.extract_old(csv_file)\n",
    "#         p = self.all_model.predict(f)[0]\n",
    "#         l = self.all_imap[p]\n",
    "#         return l\n",
    "\n",
    "#     def bin_and_all_predict(self, csv_file):\n",
    "#         f = self.extract_old(csv_file)\n",
    "#         pb = self.bin_model.predict(f)[0]\n",
    "#         lb = self.bin_imap[pb]\n",
    "#         pa = self.all_model.predict(f)[0]\n",
    "#         la = self.all_imap[pa]\n",
    "#         return lb, la\n",
    "    \n",
    "    def bin_and_all_predict(self, X):\n",
    "        pb = self.bin_model.predict(X)\n",
    "        lb = [self.bin_imap[p] for p in pb]\n",
    "        pa = self.all_model.predict(X)\n",
    "        la = [self.all_imap[p] for p in pa]\n",
    "        return lb, la\n",
    "    \n",
    "    def bin_predict(self, X, y):\n",
    "        y_pred = self.bin_model.predict(X)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        print(\"accuracy: {:.4f}\".format(accuracy))\n",
    "        evaluate(y, y_pred, 'bin')\n",
    "        cm = confusion_matrix(y, y_pred, labels=self.bin_cls)\n",
    "        return cm\n",
    "        \n",
    "    def all_predict(self, X, y):\n",
    "        y_pred = self.all_model.predict(X)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        print(\"accuracy: {:.4f}\".format(accuracy))\n",
    "        evaluate(y, y_pred, 'all')\n",
    "        cm = confusion_matrix(y, y_pred, labels=self.all_cls)\n",
    "        return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8086\n",
      "NILM 917  recall = 0.8310 precision = 0.9442\n",
      "ABN 128  recall = 0.6484 precision = 0.3487\n",
      "accuracy: 0.8545\n",
      "ACTINO 0  recall = 0.0000 precision = 0.0000\n",
      "AGC 0  recall = 0.0000 precision = 0.0000\n",
      "ASCH 7  recall = 0.0000 precision = 0.0000\n",
      "ASCUS 58  recall = 0.2586 precision = 0.2308\n",
      "CC 18  recall = 0.7778 precision = 0.7368\n",
      "EC 2  recall = 0.0000 precision = 0.0000\n",
      "CANDIDA 22  recall = 0.5000 precision = 0.4783\n",
      "HSIL 2  recall = 1.0000 precision = 0.2000\n",
      "HSV 0  recall = 0.0000 precision = 0.0000\n",
      "LSIL 7  recall = 0.7143 precision = 0.5000\n",
      "NILM 917  recall = 0.9128 precision = 0.9352\n",
      "SCC 1  recall = 0.0000 precision = 0.0000\n",
      "TRI 11  recall = 0.8182 precision = 0.6429\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_no</th>\n",
       "      <th>diagnosis_b</th>\n",
       "      <th>diagnosis_m</th>\n",
       "      <th>label_b</th>\n",
       "      <th>label_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TC19011610</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TC19014704</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>ABN</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TC19010360</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TC19010258</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TC19014744</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>ABN</td>\n",
       "      <td>CANDIDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TC19005059</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TC19005411</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TC19010375</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TC19012698</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TC19012821</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "      <td>NILM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      case_no diagnosis_b diagnosis_m label_b  label_m\n",
       "0  TC19011610        NILM        NILM    NILM     NILM\n",
       "1  TC19014704        NILM        NILM     ABN       CC\n",
       "2  TC19010360        NILM        NILM    NILM     NILM\n",
       "3  TC19010258        NILM        NILM    NILM     NILM\n",
       "4  TC19014744        NILM        NILM     ABN  CANDIDA\n",
       "5  TC19005059        NILM        NILM    NILM     NILM\n",
       "6  TC19005411        NILM        NILM    NILM     NILM\n",
       "7  TC19010375        NILM        NILM    NILM     NILM\n",
       "8  TC19012698        NILM        NILM    NILM     NILM\n",
       "9  TC19012821        NILM        NILM    NILM     NILM"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_file = \"train15models.pkl\"\n",
    "d = Diagnoser(pkl_file)\n",
    "\n",
    "b_cm = d.bin_predict(X, yb)\n",
    "a_cm = d.all_predict(X, ya)\n",
    "\n",
    "data_p = {'case_no':[], 'diagnosis_b':[], 'diagnosis_m':[], 'label_b':[], 'label_m':[]}\n",
    "lb, la = d.bin_and_all_predict(X)\n",
    "data_p['case_no'] = [os.path.basename(n).split('_BATCH')[0] for n in names]\n",
    "data_p['diagnosis_b'] = lb\n",
    "data_p['diagnosis_m'] = la\n",
    "data_p['label_b'] = [bin_imap[i] for i in yb]\n",
    "data_p['label_m'] = [all_imap[i] for i in ya]    \n",
    "\n",
    "df_p = pd.DataFrame(data=data_p)\n",
    "df_p.to_csv('train15p.csv')\n",
    "df_p.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_cm(cm, label_imap, csv_name):\n",
    "    l = len(label_imap)\n",
    "    header = [label_imap[i] for i in range(l)]\n",
    "    with open(csv_name, 'w') as csvf:\n",
    "        writer = csv.writer(csvf, delimiter=',')\n",
    "        writer.writerow(['-']+header)\n",
    "        for i in range(l):\n",
    "            writer.writerow([header[i]] + list(cm[i,:]))\n",
    "    \n",
    "write_cm(b_cm, bin_imap, 'cm-train15bin.csv')\n",
    "write_cm(a_cm, all_imap, 'cm-train15all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9127589967284624 0.453125 0.08724100327153762 0.8679425837320575 0.546875\n"
     ]
    }
   ],
   "source": [
    "TP = len(df_p[(df_p.diagnosis_m != 'NILM') & (df_p.label_m != 'NILM')])\n",
    "FN = len(df_p[(df_p.diagnosis_m == 'NILM') & (df_p.label_m != 'NILM')])\n",
    "FP = len(df_p[(df_p.diagnosis_m != 'NILM') & (df_p.label_m == 'NILM')])\n",
    "TN = len(df_p[(df_p.diagnosis_m == 'NILM') & (df_p.label_m == 'NILM')])\n",
    "\n",
    "paiyin = TN / (TN + FP)\n",
    "jiayin = FN / (TP + FN)\n",
    "jiayang = FP / (TN + FP)\n",
    "accuracy = (TN + TP) / (TP + FN + FP + TN)\n",
    "sensitivity = TP / (TP + FN)\n",
    "print(paiyin, jiayin, jiayang, accuracy, sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "nono = df_p[df_p.diagnosis_m != df_p.label_m]\n",
    "nono.case_no.to_csv('nono.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(mode=\"bin\"):  # mode = \"bin\" or \"all\"\n",
    "    seed = 2018\n",
    "    \n",
    "    y = yb if mode == \"bin\" else ya\n",
    "    num_folds = 5\n",
    "    X_train_folds = np.array_split(X, num_folds)\n",
    "    y_train_folds = np.array_split(y, num_folds)\n",
    "    \n",
    "    best = [0.0, None] # accuracy, i\n",
    "    for i in range(0, num_folds):\n",
    "        X_train = np.concatenate(X_train_folds[:i] + X_train_folds[i+1:])\n",
    "        y_train = np.concatenate(y_train_folds[:i] + y_train_folds[i+1:])\n",
    "        X_valid = X_train_folds[i]\n",
    "        y_valid = y_train_folds[i]\n",
    "\n",
    "        model = XGBClassifier(max_depth=15, \n",
    "                              n_jobs=24, \n",
    "                              subsample=0.8, \n",
    "                              colsample_bylevel=1,\n",
    "                              colsample_bytree=0.6, \n",
    "                              scale_pos_weight=1, \n",
    "                              n_estimators=500, \n",
    "                              min_child_weight=1, \n",
    "                              learning_rate=0.1, \n",
    "                              gamma=0,\n",
    "                              random_state=seed)\n",
    "        eval_set = [(X_train, y_train), (X_valid, y_valid)]\n",
    "        if mode == \"bin\":\n",
    "            eval_metric = [\"auc\", \"error\"]\n",
    "        else:\n",
    "            eval_metric = [\"merror\"]\n",
    "        model.fit(X_train, y_train, \n",
    "                  early_stopping_rounds=50, \n",
    "                  eval_metric=eval_metric, \n",
    "                  eval_set=eval_set, \n",
    "                  verbose=False)\n",
    "\n",
    "        y_pred = model.predict(X_valid)\n",
    "        accuracy = accuracy_score(y_valid, y_pred)\n",
    "        print(\"accuracy: {:.4f}\".format(accuracy))\n",
    "        \n",
    "        if accuracy > best[0]:\n",
    "            best = [accuracy, i]\n",
    "            \n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('binary classification')\n",
    "classify(\"bin\")\n",
    "print()\n",
    "print('multilabel classification')\n",
    "classify(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
