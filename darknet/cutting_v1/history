小图切割算法(从4096或tif)：
    从图片左上角开始，取1216x1216窗口，步长200，记下包含标记的小图。删掉具有包含关系的窗口(包含关系指，两个窗口包含相同标记，或一个窗口包含的标记在另一个窗口中)，输出剩下的。
    缺陷：不能排除两个窗口包含的标记有部分是重复的

广州服务器:
1. 五分类 (90%:10%随机分配) 
2. 六分类 (90%:10%随机分配) guangzhou/yolo-test2-six
3. 十四分类 (90%:10%随机分配) guangzhou/yolo-test3-fourteen
4. 六分类 (90%:10%按tif分配) guangzhou/yolo-test4-six

数据备份：
001/bigtest2：按14个分类存储1216，标记数超过3000的已被删至3000左右
001/1216_origin, 000/tct_data1216: 按14个分类存储1216， 原始数据
001/bigtest5：608x608，原始数据
001/enhancement01/three_dirs_backup：1216x1216, 数据备份，train/validate/test，没做数据增强。
001/Li/slice/4096：在线版第一批4096数据

608数据备份：
000/sakulaki/dataset/tct_data608: 608x608，原始数据，含有14分类
000/sakulaki/dataset/tct_data608_5classes： 608x608，原始数据，含有ascus/lsil/asch/hsil/scc五分类
000/sakulaki/dataset/tct_data608_5classes_rotated： 608x608，train数据做了旋转操作
000/sakulaki/dataset/tct_data608_5classes_rotated_resizedshifted： 608x608，train数据做了旋转/平移/缩放操作，每类数据量增强为ascus(8万)/lsil(8万)/asch(4万)/hsil(16万)/scc(4万)

test2数据备份：
000/sakulaki/dataset/test2: 608x608，在线版第一批标注数据，主要含有ascus/lsil两类

清洗过的608数据备份：
000/sakulaki/dataset/tct_data608_0716: 608x608，原始数据，含有14分类
000/sakulaki/dataset/tct_data608_0716_5classes： 608x608，原始数据，含有ascus/lsil/asch/hsil/scc五分类，分为train/validate/test三个文件夹
000/sakulaki/dataset/tct_data608_0716_5classes_rotated： 608x608，train数据做了旋转操作，共train/validate/test三个文件夹
000/sakulaki/dataset/tct_data608_0716_5classes_rotated_resizedshifted： 608x608，train数据做了旋转/平移/缩放操作，每类数据量增强为ascus(8万)/lsil(8万)/asch(4万)/hsil(16万)/scc(4万)
000/sakulaki/dataset/tct_data608_0716_14classes: 608x608，原始数据，含有14分类，分为train/validate/test三个文件夹
001/sakulaki/yolo-yuli/tct_data608_0716_14classes_rotated: 608x608，含有14分类，分为train/validate/test三个文件夹，train做了旋转增强

test4数据备份 (清洗过的test3)
000/sakulaki/dataset/test4: 608x608，在线版第一批审核过的数据，已清洗过，主要含有ascus/lsil/actino三类

test4_images数据备份（清洗过的test3)
tsimage-x/sakulaki/dataset/test4_images: 单个标记的抠图，大小随标记而定


1. 第一次实验(14分类）
    总共11821张图(1216x1216)，把小图随机分90%/10%为train/validate
    
2. 第二次实验(14分类)
    总共11821张图(1216x1216)，分为三个数据集：train(train做了三次旋转操作，数据变为四倍), validate和test。
        train + validate + test = 11821
        (train + validate) : test = 90% : 10%  (来自不同tif。分数据时为确保各分类都有覆盖，按诊断，如果该诊断片子数少于10张，取1张用作test；其他的取10%)
        train : validate = 90% : 10% (来自相同tif，小图随机分配)
        
3. 第三次实验(14分类) 001/bigtest4
    总共11821张图(1216x1216)，分为三个数据集：train, validate和test。
        train + validate + test = 11821
        (train + validate) : test = 90% : 10%  (来自不同tif。分数据时为确保各分类都有覆盖，按诊断，如果该诊断片子数少于10张，取1张用作test；其他的取10%)
        train : validate = 90% : 10% (来自相同tif，小图随机分配) 
        train：数据做了旋转操作
    又另加了test2，用线上标注数据做测试
    
4. 第四次实验(5分类) 001/enhancement01  1216x1216
    在enhancement01下有：
    train/validate/test三个数据集，train和validate来自相同tif，它们和test来自不同tif。其中train做了数据增强：每份原始数据做了三次旋转和一次亮度操作（数据量变为原来的五倍）；对5个分类，每个分类的标记进行缩放或者平移变换，直到每类总数达到100000个。缺点：对于一个1216包含多个同类标记的情形，每对一个标记进行变换就存一次图片，会导致这类图片比只含一个标记的图片的数量多，影响数据多样性。
    
    three_dirs_backup：数据备份，train/validate/test，没做数据增强。
    new_data：重做数据增强。区别在于，进行缩放或平移变换使每类标记数量填充到100000时，对于单个1216包含多个同类标记的图片，对该类所有标记进行缩放或平移，只存一次图片。
    这次实验使用new_data的数据
    
5. 第五次实验(14分类) 000/enhancement02  608x608
    train/validate/test，train做了旋转，14分类训练。另有test2做新样本测试
    旋转有点问题，已弃用
    
6. 第六次实验(1分类) 000/single_class_ascus 1216x1216
    train/validate/test，只挑选含有ascus的数据，train没做旋转操作
    
7. 第七次实验(14分类) 000/enhancement03 608x608
    train/validate/test，train未做旋转，14分类训练。另有test2做新样本测试

8. 第八次实验(5分类) 002/enhancement_608_01 608x608
    train/validate/test，train做旋转，5分类训练。

9. 第九次实验(5分类) 001/enhancement_608_02 608x608
    train/validate/test, train做了旋转/平移/缩放，5分类训练。


十四分类训练 (90%:10%随机分配，使用整理过的asap_608的数据) 001:/home/sakulaki/yolo-yuli/tct_data608_0716_14classes_rotated
nohup ./darknet detector train cfg/


算法：模型诊断
1. 对每一张片子进行分割(darknet)/分类(Xception)算法预测，获取坐标以及分类数据
已标记数据分配：  
    x: 02_LSIL(前半部分，已完成)
    001: 01_ASCUS(已完成)
    w: 04_HSIL(已完成)
    002: 03_ASCH(已完成，接着做02_LSIL后半部分，已完成)
    w: 05_SCC(已完成)
未标记数据分配：
    w: AWS上的4类和normal
    广州002： 2018-06-11-normal  75张

2. 从分割算法得到中间数据开始，用inception进行预测
    w： AWS上normal，共92张，现在跑normal，跑完64张，在Downloads/NORMAL_AWS
    002: 有02_LSIL和03_ASCH，现在跑02_LSIL
    w: 04_HSIL 有112张，在/home/tsimage-w/TCTData/04_HSIL
    广州002： 已跑完normal，在/media/DATA/2018-06-11-normal
              现在继续跑normal，在/media/DATA/2018-06-12-normal， 直接用inception跑全流程
    002: 02_LSIL, 03_ASCH  拷到了tsimage-y tsimage-y: 192.168.1.37:/Documents  02_LSIL_jpg 03_ASCH_jpg

    需要的数据文件夹一般以_jpg结尾，所需要的文件一般以_c.csv或者_c2.csv结尾，_c2.csv是第二次跑第二阶段生成的数据，比如第一次用xception，会生成_c.csv；第二次换成inception，会生成_c2.csv.
    查看一个文件夹中是否存在某类文件，可以用find指令：
        find /dir/path/ -name "*_c.csv"
    对文件计数：
        find /dir/path/ -name "*_c.csv" | wc -l

    001: 192.168.1.180
        /home/sakulaki/sakulaki/yolo-yuli/last_step:
            01_ASCUS_jpg  505 这个文件已经跑了inception共219个，正在跑
            ASCH_jpg  140
            HSIL_jpg  34
            LSIL_jpg  75
    002: 192.168.1.11 这台服务器产品在用，需要拷出来
        /home/tsimage/Documents
            02_LSIL_jpg  135 
            03_ASCH_jpg  55
    w: 192.168.1.192
        /home/tsimage-w/TCTData
            04_HSIL_jpg  112 已经跑了88个，正在跑
            05_SCC_jpg   40
        /home/tsimage-w/Downloads/NORMAL-AWS
            NORMAL_AWS_jpg  92
    x: 192.168.1.123
        /home/sakulaki/algo-final
            02_LSIL_jpg  172
    广州002: 192.168.0.149
        2018-06-11-normal
        2018-06-12-normal


    