{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import lmdb\n",
    "import caffe\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_txt(txt_name):\n",
    "    name_labels = []\n",
    "    with open(txt_name, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            name, label = line.strip().split()\n",
    "            label = int(label)\n",
    "            name_labels.append([name, label])\n",
    "    return name_labels\n",
    "\n",
    "def parse_dir(dir_name):\n",
    "    name_labels = []\n",
    "    classes = os.listdir(dir_name)\n",
    "    classes.sort()\n",
    "    for i,class_i in enumerate(classes):\n",
    "        names = [f for f in os.listdir(os.path.join(dir_name, class_i)) if f.endswith(\".jpg\")]\n",
    "        for name in names:\n",
    "            name = os.path.join(dir_name, class_i, name)\n",
    "            name_labels.append([name, i])\n",
    "    return name_labels\n",
    "\n",
    "def read_image(img_name, size=224):\n",
    "    img = cv2.imread(img_name)\n",
    "    img = cv2.resize(img, (size, size))\n",
    "    img = np.transpose(img, (2, 0, 1)) # hwc to chw\n",
    "    return img\n",
    "\n",
    "def make_lmdb(data_src, lmdb_name, append=False, size=224):\n",
    "    if os.path.isfile(data_src):\n",
    "        name_labels = parse_txt(data_src)\n",
    "    elif os.path.isdir(data_src):\n",
    "        name_labels = parse_dir(data_src)\n",
    "    else:\n",
    "        print(\"invalid data source.\")\n",
    "        return\n",
    "    \n",
    "    random.shuffle(name_labels)\n",
    "    random.shuffle(name_labels)\n",
    "    random.shuffle(name_labels)\n",
    "    \n",
    "    N = len(name_labels)\n",
    "    map_size = N * 3 * size * size * 3  # last 3 for redundancy purposes\n",
    "    # map_size = 137438953472 # 1 TB, still good for use\n",
    "    \n",
    "    env = lmdb.open(lmdb_name, map_size=map_size)\n",
    "    \n",
    "    if append:\n",
    "        max_key = env.stat()[\"entries\"] # assume all str_ids are consecutive and start from 0,1,2,3,...\n",
    "        # # a more thorough approach\n",
    "        # max_key = 0\n",
    "        # for key, value in env.cursor():\n",
    "        #     max_key = max(max_key, key)\n",
    "        \n",
    "        # reopen env with bigger size\n",
    "        env.close()\n",
    "        map_size = (max_key + N) * 3 * size * size * 3\n",
    "        env = lmdb.open(lmdb_name, map_size=map_size)\n",
    "        \n",
    "        print(\"Existed num of samples: \", max_key)\n",
    "    else:\n",
    "        max_key = 0\n",
    "        print(\"Create new lmdb.\")\n",
    "    \n",
    "    with env.begin(write=True) as txn:\n",
    "        # txn is a Transaction object\n",
    "        for i,name_label in enumerate(name_labels):\n",
    "            name, label = name_label\n",
    "            img = read_image(name, size)\n",
    "            \n",
    "            datum = caffe.proto.caffe_pb2.Datum()\n",
    "            datum.channels = 3\n",
    "            datum.height = size\n",
    "            datum.width = size\n",
    "            datum.data = img.tobytes()  # or .tostring() if numpy < 1.9\n",
    "            datum.label = label\n",
    "            str_id = '{:08}'.format(i + max_key)\n",
    "\n",
    "            # The encode is only essential in Python 3\n",
    "            txn.put(str_id.encode('ascii'), datum.SerializeToString())\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print(datetime.now(), \" --> \", i)\n",
    "\n",
    "                \n",
    "def get_num_records(lmdb_name):\n",
    "    env = lmdb.open(lmdb_name)\n",
    "    max_key = env.stat()[\"entries\"] # assume all str_ids are consecutive and start from 0,1,2,3,...\n",
    "    env.close()\n",
    "    return max_key\n",
    "\n",
    "\n",
    "def delete_from_lmdb(lmdb_name, keys, size=224):\n",
    "    N = get_num_records(lmdb_name)\n",
    "    map_size = N * 3 * size * size * 3  # last 3 for redundancy purposes\n",
    "    env = lmdb.open(lmdb_name, map_size=map_size)\n",
    "    with env.begin(write=True) as txn:\n",
    "        for key in keys:\n",
    "            str_id = '{:08}'.format(key).encode('ascii')\n",
    "            status = txn.delete(str_id)\n",
    "            if not status:\n",
    "                print(\"Does not exist:\", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_src = \"/home/hdd0/Data/caffe-testdata/dataset_kaggledogvscat/test_set\"\n",
    "lmdb_name = \"/home/hdd0/Data/caffe-testdata/dataset_kaggledogvscat/test_set-lmdb\"\n",
    "\n",
    "%time make_lmdb(data_src, lmdb_name, append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1994\n",
      "Does not exist: 2\n",
      "Does not exist: 5\n",
      "Does not exist: 19\n",
      "Does not exist: 32\n",
      "Does not exist: 35\n",
      "Does not exist: 85\n",
      "1994\n"
     ]
    }
   ],
   "source": [
    "lmdb_name = \"/home/hdd0/Data/caffe-testdata/dataset_kaggledogvscat/test_set-lmdb\"\n",
    "print(get_num_records(lmdb_name))\n",
    "\n",
    "keys = [2, 5, 19, 32, 35, 85]\n",
    "delete_from_lmdb(lmdb_name, keys)\n",
    "\n",
    "print(get_num_records(lmdb_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "# Let's pretend this is interesting data\n",
    "X = np.random.randint(low=0, high=255, size=(N, 3, 32, 32), dtype=np.uint8)\n",
    "y = np.random.randint(low=0, high=1, size=N, dtype=np.int64)\n",
    "\n",
    "# We need to prepare the database for the size. We'll set it 10 times\n",
    "# greater than what we theoretically need. There is little drawback to\n",
    "# setting this too big. If you still run into problem after raising\n",
    "# this, you might want to try saving fewer entries in a single\n",
    "# transaction.\n",
    "map_size = X.nbytes * 3\n",
    "print(X.nbytes, X.shape)\n",
    "\n",
    "env = lmdb.open('mylmdb', map_size=map_size)\n",
    "\n",
    "with env.begin(write=True) as txn:\n",
    "    # txn is a Transaction object\n",
    "    for i in range(N):\n",
    "        datum = caffe.proto.caffe_pb2.Datum()\n",
    "        datum.channels = X.shape[1]\n",
    "        datum.height = X.shape[2]\n",
    "        datum.width = X.shape[3]\n",
    "        datum.data = X[i].tobytes()  # or .tostring() if numpy < 1.9\n",
    "        datum.label = int(y[i])\n",
    "        str_id = '{:08}'.format(i)\n",
    "\n",
    "        # The encode is only essential in Python 3\n",
    "        txn.put(str_id.encode('ascii'), datum.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = lmdb.open('mylmdb', readonly=True)\n",
    "# with env.begin() as txn:\n",
    "#     raw_datum = txn.get(b'00000000')\n",
    "\n",
    "# datum = caffe.proto.caffe_pb2.Datum()\n",
    "# datum.ParseFromString(raw_datum)\n",
    "\n",
    "# flat_x = np.fromstring(datum.data, dtype=np.uint8)\n",
    "# x = flat_x.reshape(datum.channels, datum.height, datum.width)\n",
    "# y = datum.label\n",
    "# print(x.shape, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value0, value1000 = None, None\n",
    "with env.begin() as txn:\n",
    "    cursor = txn.cursor()\n",
    "    for key, value in cursor:\n",
    "        print(key)\n",
    "        if key == b'00000000':\n",
    "            print(type(key), key)\n",
    "            value0 = value\n",
    "        if key == b'00001000':\n",
    "            print(type(key), key)\n",
    "            value1000 = value\n",
    "    \n",
    "print(type(value0), type(value1000))\n",
    "print(value0 == value1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = lmdb.open('mylmdb', map_size=map_size)\n",
    "\n",
    "with env.begin(write=True) as txn:\n",
    "    # txn is a Transaction object\n",
    "    for i in range(N):\n",
    "        datum = caffe.proto.caffe_pb2.Datum()\n",
    "        datum.channels = X.shape[1]\n",
    "        datum.height = X.shape[2]\n",
    "        datum.width = X.shape[3]\n",
    "        datum.data = X[i].tobytes()  # or .tostring() if numpy < 1.9\n",
    "        datum.label = int(y[i])\n",
    "        str_id = '{:08}'.format(i+1000)\n",
    "\n",
    "        # The encode is only essential in Python 3\n",
    "        txn.put(str_id.encode('ascii'), datum.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
